{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v202b: Beignet Private h=128 (方向 2)\n",
    "\n",
    "**目的:** Private 模型從 h=64 升級到 h=128\n",
    "\n",
    "**背景:**\n",
    "- Beignet Public h=64→128 帶來 -1,978 (v200)\n",
    "- Private 目前還是 h=64, L=3, 1-feat, dropout=0.3\n",
    "- Private 數據量小 (82+76=158 samples)，需要更強 regularization\n",
    "\n",
    "**計劃:**\n",
    "1. Architecture sweep: h=64 vs h=128 (with 1-feat and 9-feat)\n",
    "2. Multi-seed training with best config\n",
    "3. 使用 combined normalization (v200b 驗證過比 per-domain 好)\n",
    "\n",
    "**預估效果:** Priv1 36,780→~36,200 (-580), Priv2 38,000→~37,500 (-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/Hackathon_NSF_Neural_Forecasting'\n",
    "DATA_DIR = f'{PROJECT_ROOT}/1_data/raw/train_data_neuro'\n",
    "TEST_DIR = f'{PROJECT_ROOT}/1_data/raw/test_dev_input'\n",
    "\n",
    "import os, time, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda': print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load Private Training Data + Compute Proper Normalization\n",
    "# ============================================================================\n",
    "\n",
    "train_priv1 = np.load(f'{DATA_DIR}/train_data_beignet_2022-06-01_private.npz')['arr_0']\n",
    "train_priv2 = np.load(f'{DATA_DIR}/train_data_beignet_2022-06-02_private.npz')['arr_0']\n",
    "print(f'Priv1: {train_priv1.shape}')  # (82, 20, 89, 9)\n",
    "print(f'Priv2: {train_priv2.shape}')  # (76, 20, 89, 9)\n",
    "\n",
    "# Combined private normalization (v200b Step 6 confirmed this is better)\n",
    "priv1_input = train_priv1[:, :10, :, :].astype(np.float32)\n",
    "priv2_input = train_priv2[:, :10, :, :].astype(np.float32)\n",
    "priv_all_input = np.concatenate([priv1_input, priv2_input], axis=0)\n",
    "mean_priv = priv_all_input.mean(axis=(0,1), keepdims=True)\n",
    "std_priv = priv_all_input.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "print(f'Combined Private std (feat0) avg: {std_priv[...,0].mean():.1f}')\n",
    "print(f'Total private samples: {len(priv_all_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TCN Architecture (same as v200b)\n",
    "# ============================================================================\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation)\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out[:, :, :-self.padding] if self.padding > 0 else out\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation)\n",
    "        self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation)\n",
    "        self.norm1, self.norm2 = nn.BatchNorm1d(out_ch), nn.BatchNorm1d(out_ch)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        r = self.residual(x)\n",
    "        x = self.dropout(self.activation(self.norm1(self.conv1(x))))\n",
    "        x = self.dropout(self.activation(self.norm2(self.conv2(x))))\n",
    "        return x + r\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, in_size, h_size, n_layers=4, k_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(in_size, h_size, 1)\n",
    "        self.layers = nn.ModuleList([TCNBlock(h_size, h_size, k_size, 2**i, dropout) for i in range(n_layers)])\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x.transpose(1,2))\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x.transpose(1,2)\n",
    "\n",
    "class TCNForecaster(nn.Module):\n",
    "    def __init__(self, n_ch, n_feat=1, h=64, n_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.channel_embed = nn.Embedding(n_ch, h//4)\n",
    "        self.input_proj = nn.Linear(n_feat + h//4, h)\n",
    "        self.tcn = TCNEncoder(h, h, n_layers, 3, dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(h, 4, dropout=dropout, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(h)\n",
    "        self.pred_head = nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.Dropout(dropout), nn.Linear(h,10))\n",
    "    def forward(self, x):\n",
    "        B,T,C,F = x.shape\n",
    "        ch_emb = self.channel_embed(torch.arange(C, device=x.device)).unsqueeze(0).unsqueeze(0).expand(B,T,-1,-1)\n",
    "        x = torch.cat([x, ch_emb], -1).permute(0,2,1,3).reshape(B*C,T,-1)\n",
    "        x = self.tcn(self.input_proj(x))\n",
    "        x = x[:,-1,:].view(B,C,-1)\n",
    "        x = self.attn_norm(x + self.cross_attn(x,x,x)[0])\n",
    "        return self.pred_head(x).transpose(1,2)\n",
    "\n",
    "# Count params\n",
    "for h, nl, nf in [(64, 3, 1), (128, 3, 1), (128, 3, 9), (128, 4, 1)]:\n",
    "    m = TCNForecaster(89, nf, h, nl, 0.3)\n",
    "    n = sum(p.numel() for p in m.parameters())\n",
    "    print(f'h={h}, L={nl}, feat={nf}: {n:,} params ({n/1e6:.2f}M)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Training Function (enhanced for small data)\n",
    "# ============================================================================\n",
    "\n",
    "def train_private_model(train_data, domain_mean, domain_std,\n",
    "                        n_feat, h_size, n_layers, dropout,\n",
    "                        seed=42, epochs=200, patience=25, batch_size=16, lr=5e-4):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    X_all = train_data[:, :10, :, :n_feat].astype(np.float32)\n",
    "    Y_all = train_data[:, 10:, :, 0].astype(np.float32)\n",
    "\n",
    "    dm = domain_mean[..., :n_feat]\n",
    "    ds = domain_std[..., :n_feat]\n",
    "    X_all_n = (X_all - dm) / ds\n",
    "    Y_all_n = (Y_all - domain_mean[..., 0]) / domain_std[..., 0]\n",
    "\n",
    "    n = len(X_all_n)\n",
    "    idx = np.random.permutation(n)\n",
    "    n_val = max(8, n // 6)\n",
    "    X_tr, X_val = X_all_n[idx[n_val:]], X_all_n[idx[:n_val]]\n",
    "    Y_tr, Y_val = Y_all_n[idx[n_val:]], Y_all_n[idx[:n_val]]\n",
    "\n",
    "    train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "    val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(Y_val))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    model = TCNForecaster(89, n_feat, h_size, n_layers, dropout).to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_val, best_state, no_improve = float('inf'), None, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = ((pred - yb)**2).mean()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += ((model(xb) - yb)**2).sum().item()\n",
    "        val_mse = (val_loss / len(X_val)) * (domain_std[...,0]**2).mean()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience: break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f'  seed={seed} h={h_size} L={n_layers} feat={n_feat} do={dropout}'\n",
    "          f' -> Val MSE: {best_val:.0f} ({n_params:,} params, {n} samples, ep {epoch+1}, {elapsed:.0f}s)')\n",
    "    return best_val, best_state\n",
    "\n",
    "print('Training function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Architecture Sweep (h=64 vs h=128, 1-feat vs 9-feat)\n",
    "# Use combined normalization\n",
    "# ============================================================================\n",
    "\n",
    "configs = [\n",
    "    # (n_feat, h_size, n_layers, dropout, label)\n",
    "    (1,  64,  3, 0.30, '1f h=64 L=3 do=0.3 (v200b baseline)'),\n",
    "    (1, 128,  3, 0.30, '1f h=128 L=3 do=0.3'),\n",
    "    (1, 128,  3, 0.40, '1f h=128 L=3 do=0.4'),\n",
    "    (1, 128,  3, 0.50, '1f h=128 L=3 do=0.5'),\n",
    "    (9,  64,  3, 0.30, '9f h=64 L=3 do=0.3'),\n",
    "    (9, 128,  3, 0.40, '9f h=128 L=3 do=0.4'),\n",
    "]\n",
    "\n",
    "print('=== Architecture Sweep (seed=42, combined normalization) ===')\n",
    "sweep_results = []\n",
    "\n",
    "for nf, h, nl, do, label in configs:\n",
    "    print(f'\\n--- {label} ---')\n",
    "    v1, s1 = train_private_model(train_priv1, mean_priv, std_priv, nf, h, nl, do, seed=42)\n",
    "    v2, s2 = train_private_model(train_priv2, mean_priv, std_priv, nf, h, nl, do, seed=42)\n",
    "    avg = (v1 + v2) / 2\n",
    "    sweep_results.append((nf, h, nl, do, label, v1, v2, avg))\n",
    "    print(f'  Combined: {avg:.0f}')\n",
    "\n",
    "sweep_results.sort(key=lambda x: x[7])\n",
    "print(f'\\n=== Results (sorted by combined MSE) ===')\n",
    "for nf, h, nl, do, label, v1, v2, avg in sweep_results:\n",
    "    tag = ' *** BEST' if avg == sweep_results[0][7] else ''\n",
    "    print(f'{label}: P1={v1:.0f} P2={v2:.0f} Avg={avg:.0f}{tag}')\n",
    "\n",
    "best = sweep_results[0]\n",
    "BEST_NF, BEST_H, BEST_NL, BEST_DO = best[0], best[1], best[2], best[3]\n",
    "print(f'\\nBest config: feat={BEST_NF}, h={BEST_H}, L={BEST_NL}, do={BEST_DO}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Multi-Seed Training with Best Config\n",
    "# ============================================================================\n",
    "\n",
    "seeds = [42, 123, 456, 789, 2024]\n",
    "\n",
    "print(f'=== Multi-Seed (feat={BEST_NF}, h={BEST_H}, L={BEST_NL}, do={BEST_DO}) ===')\n",
    "\n",
    "print('\\n--- Priv1 ---')\n",
    "p1_results = []\n",
    "for s in seeds:\n",
    "    v, state = train_private_model(train_priv1, mean_priv, std_priv, BEST_NF, BEST_H, BEST_NL, BEST_DO, seed=s)\n",
    "    p1_results.append((s, v, state))\n",
    "\n",
    "print('\\n--- Priv2 ---')\n",
    "p2_results = []\n",
    "for s in seeds:\n",
    "    v, state = train_private_model(train_priv2, mean_priv, std_priv, BEST_NF, BEST_H, BEST_NL, BEST_DO, seed=s)\n",
    "    p2_results.append((s, v, state))\n",
    "\n",
    "print(f'\\n=== Results ===')\n",
    "p1_vals = [r[1] for r in p1_results]\n",
    "p2_vals = [r[1] for r in p2_results]\n",
    "print(f'Priv1: mean={np.mean(p1_vals):.0f} std={np.std(p1_vals):.0f}')\n",
    "print(f'Priv2: mean={np.mean(p2_vals):.0f} std={np.std(p2_vals):.0f}')\n",
    "for i, s in enumerate(seeds):\n",
    "    print(f'  seed={s}: P1={p1_vals[i]:.0f} P2={p2_vals[i]:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Save All Models\n",
    "# ============================================================================\n",
    "\n",
    "out_dir = f'{PROJECT_ROOT}/4_models/v202b_private_h128'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'n_feat': int(BEST_NF),\n",
    "    'h_size': int(BEST_H),\n",
    "    'n_layers': int(BEST_NL),\n",
    "    'dropout': float(BEST_DO),\n",
    "    'seeds': seeds,\n",
    "}\n",
    "print(f'Config: {config}')\n",
    "\n",
    "for s, v, state in p1_results:\n",
    "    path = f'{out_dir}/model_tcn_priv1_seed{s}.pth'\n",
    "    torch.save({'model_state_dict': state, 'val_mse': v, 'config': config}, path)\n",
    "    print(f'Priv1 seed {s}: Val={v:.0f} -> {path}')\n",
    "\n",
    "for s, v, state in p2_results:\n",
    "    path = f'{out_dir}/model_tcn_priv2_seed{s}.pth'\n",
    "    torch.save({'model_state_dict': state, 'val_mse': v, 'config': config}, path)\n",
    "    print(f'Priv2 seed {s}: Val={v:.0f} -> {path}')\n",
    "\n",
    "# Save normalization (combined)\n",
    "np.savez(f'{out_dir}/normalization_priv_combined.npz', mean=mean_priv, std=std_priv)\n",
    "print(f'\\nSaved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Download\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "for s in seeds:\n",
    "    files.download(f'{out_dir}/model_tcn_priv1_seed{s}.pth')\n",
    "    files.download(f'{out_dir}/model_tcn_priv2_seed{s}.pth')\n",
    "\n",
    "files.download(f'{out_dir}/normalization_priv_combined.npz')\n",
    "\n",
    "print('\\n=== Downloaded! ===')\n",
    "print(f'Config: feat={BEST_NF}, h={BEST_H}, L={BEST_NL}, do={BEST_DO}')\n",
    "print(f'\\nFor model.py, update:')\n",
    "print(f'  V200B_PRIV_H = {BEST_H}')\n",
    "print(f'  V200B_PRIV_LAYERS = {BEST_NL}')\n",
    "print(f'  V200B_PRIV_DROPOUT = {BEST_DO}')\n",
    "print(f'  V200B_PRIV_FEAT = {BEST_NF}')\n",
    "print(f'  V200B_PRIV_SEEDS = {seeds}')\n",
    "print(f'\\nIMPORTANT: If h changed, the .pth files are NOT compatible with old model!')\n",
    "print(f'Must replace ALL priv seed files, not just add new ones.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}