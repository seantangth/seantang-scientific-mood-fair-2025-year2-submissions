{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v192d: NHiTS with CORAL Domain Adaptation\n",
    "\n",
    "NHiTS 佔 Beignet Public ensemble 的 30% 權重。\n",
    "如果 NHiTS 也用 CORAL 訓練，可能進一步改善。\n",
    "\n",
    "NHiTS 用 1 feature (feature 0)，和 v176 完全相同架構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/Hackathon_NSF_Neural_Forecasting'\n",
    "TRAIN_DIR = f'{PROJECT_ROOT}/1_data/raw/train_data_neuro'\n",
    "TEST_DIR = f'{PROJECT_ROOT}/1_data/raw/test_dev_input'\n",
    "\n",
    "import os, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coral_loss(source, target):\n",
    "    d = source.size(1)\n",
    "    cs = (source - source.mean(0, keepdim=True)).T @ (source - source.mean(0, keepdim=True)) / (source.size(0) - 1 + 1e-8)\n",
    "    ct = (target - target.mean(0, keepdim=True)).T @ (target - target.mean(0, keepdim=True)) / (target.size(0) - 1 + 1e-8)\n",
    "    return ((cs - ct) ** 2).sum() / (4 * d)\n",
    "\n",
    "def mean_alignment_loss(source, target):\n",
    "    return ((source.mean(0) - target.mean(0)) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHiTS Architecture (和 v176 完全相同)\n",
    "\n",
    "class NHiTSBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_steps, pool_kernel_size, n_freq_downsample, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.output_steps = output_steps\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "        self.pooling = nn.MaxPool1d(kernel_size=pool_kernel_size, stride=pool_kernel_size, ceil_mode=True)\n",
    "        self.pooled_size = (input_size + pool_kernel_size - 1) // pool_kernel_size\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.pooled_size, hidden_size), nn.ReLU(), nn.Dropout(dropout),\n",
    "                                  nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Dropout(dropout))\n",
    "        self.n_coeffs = max(1, output_steps // n_freq_downsample)\n",
    "        self.backcast_proj = nn.Linear(hidden_size, input_size)\n",
    "        self.forecast_proj = nn.Linear(hidden_size, self.n_coeffs)\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        x_pooled = self.pooling(x.unsqueeze(1)).squeeze(1)\n",
    "        if x_pooled.shape[1] < self.pooled_size:\n",
    "            x_pooled = F.pad(x_pooled, (0, self.pooled_size - x_pooled.shape[1]))\n",
    "        elif x_pooled.shape[1] > self.pooled_size:\n",
    "            x_pooled = x_pooled[:, :self.pooled_size]\n",
    "        h = self.mlp(x_pooled)\n",
    "        backcast = self.backcast_proj(h)\n",
    "        forecast_coeffs = self.forecast_proj(h)\n",
    "        if self.n_coeffs < self.output_steps:\n",
    "            forecast = F.interpolate(forecast_coeffs.unsqueeze(1), size=self.output_steps, mode='linear', align_corners=False).squeeze(1)\n",
    "        else:\n",
    "            forecast = forecast_coeffs[:, :self.output_steps]\n",
    "        if return_hidden:\n",
    "            return backcast, forecast, h\n",
    "        return backcast, forecast\n",
    "\n",
    "class NHiTSStack(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_steps, n_blocks=2, pool_kernel_sizes=None, n_freq_downsamples=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pool_kernel_sizes = pool_kernel_sizes or [1, 2][:n_blocks]\n",
    "        n_freq_downsamples = n_freq_downsamples or [1, 2][:n_blocks]\n",
    "        self.blocks = nn.ModuleList([NHiTSBlock(input_size, hidden_size, output_steps, pool_kernel_sizes[i % len(pool_kernel_sizes)], n_freq_downsamples[i % len(n_freq_downsamples)], dropout) for i in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x, return_hidden=False):\n",
    "        residual, total_forecast = x, 0\n",
    "        hiddens = []\n",
    "        for block in self.blocks:\n",
    "            if return_hidden:\n",
    "                backcast, forecast, h = block(residual, return_hidden=True)\n",
    "                hiddens.append(h)\n",
    "            else:\n",
    "                backcast, forecast = block(residual)\n",
    "            residual = residual - backcast\n",
    "            total_forecast = total_forecast + forecast\n",
    "        if return_hidden:\n",
    "            return x - residual, total_forecast, hiddens\n",
    "        return x - residual, total_forecast\n",
    "\n",
    "class NHiTSForecaster(nn.Module):\n",
    "    def __init__(self, n_channels, n_features=1, hidden_size=128, num_layers=2, n_stacks=2, seq_len=10, dropout=0.1, output_steps=10):\n",
    "        super().__init__()\n",
    "        self.n_channels, self.output_steps = n_channels, output_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        self.stacks = nn.ModuleList([NHiTSStack(seq_len, hidden_size, output_steps, num_layers, [pk*(i+1) for pk in [1,2]], [fd*(i+1) for fd in [1,2]], dropout) for i in range(n_stacks)])\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        B, T, C, F = x.shape\n",
    "        x_flat = x[:,:,:,0].transpose(1,2).reshape(B*C, T)\n",
    "\n",
    "        if return_features:\n",
    "            all_hiddens = []\n",
    "            total_forecast = 0\n",
    "            for stack in self.stacks:\n",
    "                _, forecast, hiddens = stack(x_flat, return_hidden=True)\n",
    "                total_forecast = total_forecast + forecast\n",
    "                all_hiddens.extend(hiddens)\n",
    "            pred = total_forecast.view(B, C, self.output_steps).transpose(1,2)\n",
    "            # Aggregate hidden features: mean across all blocks, then reshape to (B, C, hidden) and mean across C\n",
    "            h_cat = torch.stack(all_hiddens, dim=0).mean(dim=0)  # (B*C, hidden)\n",
    "            h_sample = h_cat.view(B, C, -1).mean(dim=1)  # (B, hidden)\n",
    "            return pred, h_sample\n",
    "        else:\n",
    "            total_forecast = sum(stack(x_flat)[1] for stack in self.stacks)\n",
    "            return total_forecast.view(B, C, self.output_steps).transpose(1,2)\n",
    "\n",
    "# Verify\n",
    "m = NHiTSForecaster(89, 1, 256, 2, 2, 10, 0.1)\n",
    "print(f'NHiTS parameters: {sum(p.numel() for p in m.parameters()):,}')\n",
    "x_test = torch.randn(2, 10, 89, 1)\n",
    "pred, feat = m(x_test, return_features=True)\n",
    "print(f'Pred: {pred.shape}, Feat: {feat.shape}')\n",
    "del m, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (NHiTS uses 1 feature only)\n",
    "train_data = np.load(f'{TRAIN_DIR}/train_data_beignet.npz')['arr_0']\n",
    "test_public = np.load(f'{TEST_DIR}/test_data_beignet_masked.npz')['arr_0']\n",
    "\n",
    "X_train = train_data[:, :10, :, 0:1].astype(np.float32)  # 1 feature\n",
    "Y_train = train_data[:, 10:, :, 0].astype(np.float32)\n",
    "X_target = test_public[:, :10, :, 0:1].astype(np.float32)\n",
    "\n",
    "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
    "std = X_train.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_norm = (X_train - mean) / std\n",
    "Y_train_norm = (Y_train - mean[...,0]) / std[...,0]\n",
    "X_target_norm = (X_target - mean) / std\n",
    "\n",
    "n_val = 100\n",
    "X_tr, X_val = X_train_norm[:-n_val], X_train_norm[-n_val:]\n",
    "Y_tr, Y_val = Y_train_norm[:-n_val], Y_train_norm[-n_val:]\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(Y_val))\n",
    "target_ds = TensorDataset(torch.FloatTensor(X_target_norm))\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "target_dl = DataLoader(target_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Train: {len(X_tr)}, Val: {len(X_val)}, Target: {len(X_target_norm)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "CORAL_WEIGHT = 1.0\n",
    "MEAN_WEIGHT = 0.5\n",
    "EPOCHS = 200\n",
    "PATIENCE = 30\n",
    "\n",
    "model = NHiTSForecaster(89, 1, 256, 2, 2, 10, 0.1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "best_val = float('inf')\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "print(f'Training NHiTS with CORAL={CORAL_WEIGHT}, Mean={MEAN_WEIGHT}')\n",
    "print('-' * 70)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_mse, train_coral, n_batches = 0, 0, 0\n",
    "    target_iter = iter(target_dl)\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        try: (xt,) = next(target_iter)\n",
    "        except StopIteration:\n",
    "            target_iter = iter(target_dl)\n",
    "            (xt,) = next(target_iter)\n",
    "        xt = xt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred, feat_src = model(xb, return_features=True)\n",
    "        _, feat_tgt = model(xt, return_features=True)\n",
    "\n",
    "        mse = ((pred - yb)**2).mean()\n",
    "        coral = coral_loss(feat_src, feat_tgt)\n",
    "        mean_a = mean_alignment_loss(feat_src, feat_tgt)\n",
    "        loss = mse + CORAL_WEIGHT * coral + MEAN_WEIGHT * mean_a\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_mse += mse.item()\n",
    "        train_coral += coral.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    train_mse /= n_batches\n",
    "    train_coral /= n_batches\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_loss += ((model(xb) - yb)**2).sum().item()\n",
    "    val_mse = (val_loss / len(X_val)) * (std[...,0]**2).mean()\n",
    "\n",
    "    if val_mse < best_val:\n",
    "        best_val = val_mse\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "        print(f'Epoch {epoch+1:3d}: MSE={train_mse:.4f}, CORAL={train_coral:.6f}, Val={val_mse:.0f} ***')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch+1:3d}: MSE={train_mse:.4f}, CORAL={train_coral:.6f}, Val={val_mse:.0f}')\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "print(f'\\nBest Val MSE: {best_val:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "out_dir = f'{PROJECT_ROOT}/4_models/v192d_nhits_coral'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': best_state,\n",
    "    'val_mse': best_val,\n",
    "}, f'{out_dir}/model_nhits_coral.pth')\n",
    "\n",
    "np.savez(f'{out_dir}/normalization_beignet_nhits.npz', mean=mean, std=std)\n",
    "\n",
    "print(f'Saved to {out_dir}')\n",
    "print('model_nhits_coral.pth → 替換 v176 的 model_nhits.pth')\n",
    "print('normalization_beignet_nhits.npz → 替換 v176 的 normalization_beignet_nhits.npz')\n",
    "\n",
    "from google.colab import files\n",
    "files.download(f'{out_dir}/model_nhits_coral.pth')\n",
    "files.download(f'{out_dir}/normalization_beignet_nhits.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
