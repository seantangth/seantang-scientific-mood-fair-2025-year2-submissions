{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v200: Beignet Larger TCN + Multi-Seed + Augmentation\n",
    "\n",
    "**策略:**\n",
    "1. Scale up Beignet TCN: h=64 → h=128, 3 layers → 4 layers (104K → ~400K params)\n",
    "2. Training-time augmentation: channel shift, amplitude scale, noise (zero inference cost)\n",
    "3. Multi-seed ensemble: train 5 seeds, average predictions at inference\n",
    "\n",
    "**Baseline:** v193d/v199b Beignet Public = 55,367 (CORAL λ=5.0, h=64, 3 layers)\n",
    "**Target:** < 52,000 Beignet Public → Total MSE < 40,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/Hackathon_NSF_Neural_Forecasting'\n",
    "TRAIN_DIR = f'{PROJECT_ROOT}/1_data/raw/train_data_neuro'\n",
    "TEST_DIR = f'{PROJECT_ROOT}/1_data/raw/test_dev_input'\n",
    "\n",
    "import os, time, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda': print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CORAL Losses\n",
    "# ============================================================================\n",
    "\n",
    "def coral_loss(source, target):\n",
    "    d = source.size(1)\n",
    "    cs = (source - source.mean(0, keepdim=True)).T @ (source - source.mean(0, keepdim=True)) / (source.size(0) - 1 + 1e-8)\n",
    "    ct = (target - target.mean(0, keepdim=True)).T @ (target - target.mean(0, keepdim=True)) / (target.size(0) - 1 + 1e-8)\n",
    "    return ((cs - ct) ** 2).sum() / (4 * d)\n",
    "\n",
    "def mean_alignment_loss(source, target):\n",
    "    return ((source.mean(0) - target.mean(0)) ** 2).mean()\n",
    "\n",
    "# ============================================================================\n",
    "# TCN Architecture (same as v193d, parameterized h and n_layers)\n",
    "# ============================================================================\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation)\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out[:, :, :-self.padding] if self.padding > 0 else out\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation)\n",
    "        self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation)\n",
    "        self.norm1, self.norm2 = nn.BatchNorm1d(out_ch), nn.BatchNorm1d(out_ch)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        r = self.residual(x)\n",
    "        x = self.dropout(self.activation(self.norm1(self.conv1(x))))\n",
    "        x = self.dropout(self.activation(self.norm2(self.conv2(x))))\n",
    "        return x + r\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, in_size, h_size, n_layers=4, k_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(in_size, h_size, 1)\n",
    "        self.layers = nn.ModuleList([TCNBlock(h_size, h_size, k_size, 2**i, dropout) for i in range(n_layers)])\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x.transpose(1,2))\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x.transpose(1,2)\n",
    "\n",
    "class TCNForecaster(nn.Module):\n",
    "    def __init__(self, n_ch, n_feat=1, h=64, n_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.channel_embed = nn.Embedding(n_ch, h//4)\n",
    "        self.input_proj = nn.Linear(n_feat + h//4, h)\n",
    "        self.tcn = TCNEncoder(h, h, n_layers, 3, dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(h, 4, dropout=dropout, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(h)\n",
    "        self.pred_head = nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.Dropout(dropout), nn.Linear(h,10))\n",
    "    def forward(self, x, return_features=False):\n",
    "        B,T,C,F = x.shape\n",
    "        ch_emb = self.channel_embed(torch.arange(C, device=x.device)).unsqueeze(0).unsqueeze(0).expand(B,T,-1,-1)\n",
    "        x = torch.cat([x, ch_emb], -1).permute(0,2,1,3).reshape(B*C,T,-1)\n",
    "        x = self.tcn(self.input_proj(x))\n",
    "        x = x[:,-1,:].view(B,C,-1)\n",
    "        x = self.attn_norm(x + self.cross_attn(x,x,x)[0])\n",
    "        pred = self.pred_head(x).transpose(1,2)\n",
    "        if return_features:\n",
    "            return pred, x.mean(dim=1)\n",
    "        return pred\n",
    "\n",
    "# Count params\n",
    "for h, nl in [(64, 3), (128, 4), (192, 4)]:\n",
    "    m = TCNForecaster(89, 9, h, nl, 0.25)\n",
    "    n = sum(p.numel() for p in m.parameters())\n",
    "    print(f'h={h}, layers={nl}: {n:,} params ({n/1e6:.2f}M)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Load Data\n",
    "# ============================================================================\n",
    "\n",
    "train_data = np.load(f'{TRAIN_DIR}/train_data_beignet.npz')['arr_0']\n",
    "test_public = np.load(f'{TEST_DIR}/test_data_beignet_masked.npz')['arr_0']\n",
    "\n",
    "n_features = 9\n",
    "X_train = train_data[:, :10, :, :n_features].astype(np.float32)\n",
    "Y_train = train_data[:, 10:, :, 0].astype(np.float32)\n",
    "X_target = test_public[:, :10, :, :n_features].astype(np.float32)\n",
    "\n",
    "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
    "std = X_train.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_n = (X_train - mean) / std\n",
    "Y_train_n = (Y_train - mean[...,0]) / std[...,0]\n",
    "X_target_n = (X_target - mean) / std\n",
    "\n",
    "n_val = 100\n",
    "X_tr, X_val = X_train_n[:-n_val], X_train_n[-n_val:]\n",
    "Y_tr, Y_val = Y_train_n[:-n_val], Y_train_n[-n_val:]\n",
    "\n",
    "print(f'Train: {len(X_tr)}, Val: {len(X_val)}, Target: {len(X_target_n)}')\n",
    "print(f'X shape: {X_tr.shape}, Y shape: {Y_tr.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Training-Time Augmentation (zero inference cost)\n",
    "# ============================================================================\n",
    "\n",
    "def augment_batch(x, y):\n",
    "    \"\"\"Apply augmentations to source batch during training.\n",
    "    x: (B, T, C, F) normalized\n",
    "    y: (B, T_out, C) normalized\n",
    "    \"\"\"\n",
    "    # 1. Channel-wise mean shift (simulates baseline drift between domains)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        shift = 0.15 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x.clone()\n",
    "        x[..., 0:1] = x[..., 0:1] + shift\n",
    "        y = y + shift[..., 0].squeeze(0)  # shift target too\n",
    "\n",
    "    # 2. Amplitude scaling per channel (simulates gain drift)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        scale = 1.0 + 0.08 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x * scale\n",
    "        y = y * scale[..., 0].squeeze(0)\n",
    "\n",
    "    # 3. Gaussian noise (regularization)\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        x = x + 0.03 * torch.randn_like(x)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "print('Augmentation defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Training Function with Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "def train_coral_model(h_size, n_layers, dropout, coral_w, mean_w, seed,\n",
    "                      epochs=250, patience=30, batch_size=32, use_aug=True):\n",
    "    \"\"\"Train a single TCN CORAL model with given config and seed.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "    val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(Y_val))\n",
    "    target_ds = TensorDataset(torch.FloatTensor(X_target_n))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "    target_dl = DataLoader(target_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = TCNForecaster(89, 9, h_size, n_layers, dropout).to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_val, best_state, no_improve = float('inf'), None, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        target_iter = iter(target_dl)\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            try: (xt,) = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(target_dl)\n",
    "                (xt,) = next(target_iter)\n",
    "            xt = xt.to(device)\n",
    "\n",
    "            # Apply augmentation\n",
    "            if use_aug:\n",
    "                xb, yb = augment_batch(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, feat_src = model(xb, return_features=True)\n",
    "            _, feat_tgt = model(xt, return_features=True)\n",
    "            loss = ((pred - yb)**2).mean() + coral_w * coral_loss(feat_src, feat_tgt) + mean_w * mean_alignment_loss(feat_src, feat_tgt)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += ((model(xb) - yb)**2).sum().item()\n",
    "        val_mse = (val_loss / len(X_val)) * (std[...,0]**2).mean()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience: break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f'  seed={seed} h={h_size} L={n_layers} CORAL={coral_w} Mean={mean_w} '\n",
    "          f'-> Val MSE: {best_val:.0f} ({n_params:,} params, ep {epoch+1}, {elapsed:.0f}s)')\n",
    "    return best_val, best_state\n",
    "\n",
    "print('Training function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Architecture Sweep (find best h/layers with single seed)\n",
    "# ============================================================================\n",
    "\n",
    "arch_configs = [\n",
    "    # (h_size, n_layers, dropout, coral_w, mean_w)\n",
    "    (64,  3, 0.30, 5.0, 2.0),   # baseline (v193d config)\n",
    "    (128, 3, 0.25, 5.0, 2.0),   # wider\n",
    "    (128, 4, 0.25, 5.0, 2.0),   # wider + deeper\n",
    "    (192, 4, 0.25, 5.0, 2.0),   # even wider + deeper\n",
    "]\n",
    "\n",
    "print('=== Architecture Sweep (seed=42) ===')\n",
    "arch_results = []\n",
    "for h, nl, do, cw, mw in arch_configs:\n",
    "    val_mse, state = train_coral_model(h, nl, do, cw, mw, seed=42)\n",
    "    arch_results.append((h, nl, do, cw, mw, val_mse, state))\n",
    "\n",
    "arch_results.sort(key=lambda x: x[5])\n",
    "print(f'\\n=== Architecture Results ===')\n",
    "for h, nl, do, cw, mw, val_mse, _ in arch_results:\n",
    "    tag = ' *** BEST' if val_mse == arch_results[0][5] else ''\n",
    "    print(f'h={h} L={nl} do={do} CORAL={cw} Mean={mw} -> Val={val_mse:.0f}{tag}')\n",
    "\n",
    "best_h, best_nl, best_do = arch_results[0][0], arch_results[0][1], arch_results[0][2]\n",
    "print(f'\\nBest architecture: h={best_h}, layers={best_nl}, dropout={best_do}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: CORAL Lambda Sweep with Best Architecture\n",
    "# ============================================================================\n",
    "\n",
    "coral_configs = [\n",
    "    (3.0, 1.0),\n",
    "    (5.0, 2.0),   # v193d config\n",
    "    (5.0, 3.0),\n",
    "    (8.0, 2.0),\n",
    "    (8.0, 3.0),\n",
    "]\n",
    "\n",
    "print(f'=== CORAL Sweep with h={best_h}, L={best_nl} (seed=42) ===')\n",
    "coral_results = []\n",
    "for cw, mw in coral_configs:\n",
    "    val_mse, state = train_coral_model(best_h, best_nl, best_do, cw, mw, seed=42)\n",
    "    coral_results.append((cw, mw, val_mse, state))\n",
    "\n",
    "coral_results.sort(key=lambda x: x[2])\n",
    "print(f'\\n=== CORAL Results ===')\n",
    "for cw, mw, val_mse, _ in coral_results:\n",
    "    tag = ' *** BEST' if val_mse == coral_results[0][2] else ''\n",
    "    print(f'CORAL={cw} Mean={mw} -> Val={val_mse:.0f}{tag}')\n",
    "\n",
    "best_cw, best_mw = coral_results[0][0], coral_results[0][1]\n",
    "print(f'\\nBest CORAL config: λ={best_cw}, Mean={best_mw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Multi-Seed Training with Best Config\n",
    "# ============================================================================\n",
    "\n",
    "seeds = [42, 123, 456, 789, 2024]\n",
    "\n",
    "print(f'=== Multi-Seed Training: h={best_h}, L={best_nl}, CORAL={best_cw}, Mean={best_mw} ===')\n",
    "seed_results = []\n",
    "for s in seeds:\n",
    "    val_mse, state = train_coral_model(best_h, best_nl, best_do, best_cw, best_mw, seed=s)\n",
    "    seed_results.append((s, val_mse, state))\n",
    "\n",
    "print(f'\\n=== Seed Results ===')\n",
    "val_mses = [r[1] for r in seed_results]\n",
    "for s, val_mse, _ in seed_results:\n",
    "    print(f'seed={s} -> Val MSE: {val_mse:.0f}')\n",
    "print(f'\\nMean: {np.mean(val_mses):.0f}, Std: {np.std(val_mses):.0f}')\n",
    "print(f'Variance across seeds shows {np.std(val_mses)/np.mean(val_mses)*100:.1f}% variation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Save All Seed Checkpoints\n",
    "# ============================================================================\n",
    "\n",
    "out_dir = f'{PROJECT_ROOT}/4_models/v200_beignet_multiseed'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'h_size': int(best_h),\n",
    "    'n_layers': int(best_nl),\n",
    "    'dropout': float(best_do),\n",
    "    'coral_weight': float(best_cw),\n",
    "    'mean_weight': float(best_mw),\n",
    "    'n_features': 9,\n",
    "    'seeds': seeds,\n",
    "}\n",
    "print(f'Config: {config}')\n",
    "\n",
    "# Save each seed's checkpoint\n",
    "for s, val_mse, state in seed_results:\n",
    "    path = f'{out_dir}/model_tcn_seed{s}.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': state,\n",
    "        'val_mse': val_mse,\n",
    "        'config': config,\n",
    "    }, path)\n",
    "    fsize = os.path.getsize(path) / 1024\n",
    "    print(f'Saved seed {s}: Val={val_mse:.0f}, size={fsize:.0f}KB -> {path}')\n",
    "\n",
    "# Save normalization\n",
    "np.savez(f'{out_dir}/normalization_beignet_tcn.npz', mean=mean, std=std)\n",
    "print(f'\\nSaved normalization to {out_dir}')\n",
    "\n",
    "# Also save best single model for comparison\n",
    "best_seed_result = min(seed_results, key=lambda x: x[1])\n",
    "torch.save({'model_state_dict': best_seed_result[2]}, f'{out_dir}/model_tcn_best_single.pth')\n",
    "print(f'Best single seed: {best_seed_result[0]} (Val={best_seed_result[1]:.0f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Local Validation - Compare Single vs Multi-Seed Ensemble\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_on_val(models, X_val_t, Y_val_t):\n",
    "    \"\"\"Evaluate ensemble of models on validation set.\"\"\"\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for m in models:\n",
    "            m.eval()\n",
    "            p = m(X_val_t).cpu().numpy()\n",
    "            preds.append(p)\n",
    "    avg_pred = np.mean(preds, axis=0)\n",
    "    # Denormalize\n",
    "    avg_pred_raw = avg_pred * std[...,0] + mean[...,0]\n",
    "    y_raw = Y_val_t.cpu().numpy() * std[...,0] + mean[...,0]\n",
    "    return ((avg_pred_raw - y_raw)**2).mean()\n",
    "\n",
    "X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "Y_val_t = torch.FloatTensor(Y_val).to(device)\n",
    "\n",
    "# Load all seed models\n",
    "all_models = []\n",
    "for s, _, state in seed_results:\n",
    "    m = TCNForecaster(89, 9, best_h, best_nl, best_do).to(device)\n",
    "    m.load_state_dict(state)\n",
    "    m.eval()\n",
    "    all_models.append(m)\n",
    "\n",
    "# Single model MSEs\n",
    "print('=== Single Model Validation ===')\n",
    "for i, (s, _, _) in enumerate(seed_results):\n",
    "    mse = evaluate_on_val([all_models[i]], X_val_t, Y_val_t)\n",
    "    print(f'seed={s}: Val MSE = {mse:.0f}')\n",
    "\n",
    "# Ensemble MSEs\n",
    "print('\\n=== Ensemble Validation ===')\n",
    "for k in [2, 3, 4, 5]:\n",
    "    if k <= len(all_models):\n",
    "        mse = evaluate_on_val(all_models[:k], X_val_t, Y_val_t)\n",
    "        print(f'{k}-seed ensemble: Val MSE = {mse:.0f}')\n",
    "\n",
    "print('\\nDone! Check which ensemble size gives best result.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Download Models\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# Download all seed checkpoints\n",
    "for s in seeds:\n",
    "    files.download(f'{out_dir}/model_tcn_seed{s}.pth')\n",
    "\n",
    "# Download normalization\n",
    "files.download(f'{out_dir}/normalization_beignet_tcn.npz')\n",
    "\n",
    "print('\\nAll files downloaded!')\n",
    "print(f'\\nNext steps:')\n",
    "print(f'1. Put model_tcn_seed*.pth in submission folder')\n",
    "print(f'2. Update model.py to load {len(seeds)} TCN seeds with h={best_h}, L={best_nl}')\n",
    "print(f'3. Average predictions in model.py')\n",
    "print(f'4. Submit to Codabench')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 7 (Optional): Augmentation Ablation\n",
    "# Compare with vs without augmentation to see its contribution\n",
    "# ============================================================================\n",
    "\n",
    "print('=== Augmentation Ablation (seed=42) ===')\n",
    "val_aug, _ = train_coral_model(best_h, best_nl, best_do, best_cw, best_mw, seed=42, use_aug=True)\n",
    "val_noaug, _ = train_coral_model(best_h, best_nl, best_do, best_cw, best_mw, seed=42, use_aug=False)\n",
    "print(f'\\nWith augmentation: {val_aug:.0f}')\n",
    "print(f'Without augmentation: {val_noaug:.0f}')\n",
    "print(f'Augmentation effect: {val_aug - val_noaug:+.0f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}