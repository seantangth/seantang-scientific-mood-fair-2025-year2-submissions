{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v213: Stronger Transformer for Beignet Public\n",
    "\n",
    "**Current TF:** h=256, L=4, heads=4, do=0.2, val ~53-56K (vs TCN ~47-50K)\n",
    "\n",
    "**Goal:** Close the gap to TCN quality. Better TF = better ensemble.\n",
    "\n",
    "**Improvements to try:**\n",
    "1. Pre-LayerNorm (more stable training)\n",
    "2. Longer training (500 epochs, patience=50)\n",
    "3. Learning rate warmup (10 epochs)\n",
    "4. Deeper model (L=6)\n",
    "5. More heads (8)\n",
    "\n",
    "**Plan:** Sweep configs, then train 5 seeds with best config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/Hackathon_NSF_Neural_Forecasting'\n",
    "TRAIN_DIR = f'{PROJECT_ROOT}/1_data/raw/train_data_neuro'\n",
    "TEST_DIR = f'{PROJECT_ROOT}/1_data/raw/test_dev_input'\n",
    "\n",
    "import os, time, torch, numpy as np, math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Architecture + CORAL + Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "def coral_loss(source, target):\n",
    "    d = source.size(1)\n",
    "    cs = (source - source.mean(0, keepdim=True)).T @ (source - source.mean(0, keepdim=True)) / (source.size(0) - 1 + 1e-8)\n",
    "    ct = (target - target.mean(0, keepdim=True)).T @ (target - target.mean(0, keepdim=True)) / (target.size(0) - 1 + 1e-8)\n",
    "    return ((cs - ct) ** 2).sum() / (4 * d)\n",
    "\n",
    "def mean_alignment_loss(source, target):\n",
    "    return ((source.mean(0) - target.mean(0)) ** 2).mean()\n",
    "\n",
    "class TransformerForecaster(nn.Module):\n",
    "    \"\"\"Improved Transformer with Pre-LN for more stable training.\"\"\"\n",
    "    def __init__(self, n_ch, n_feat=1, h=256, n_layers=4, n_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.channel_embed = nn.Embedding(n_ch, h // 4)\n",
    "        self.input_proj = nn.Linear(n_feat + h // 4, h)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 10, h) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=h, nhead=n_heads, dim_feedforward=h * 4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu',\n",
    "            norm_first=True  # Pre-LN: more stable training\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.cross_attn = nn.MultiheadAttention(h, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(h)\n",
    "        self.pred_head = nn.Sequential(\n",
    "            nn.Linear(h, h), nn.GELU(), nn.Dropout(dropout), nn.Linear(h, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        B, T, C, F = x.shape\n",
    "        ch_emb = self.channel_embed(torch.arange(C, device=x.device)).unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)\n",
    "        x = torch.cat([x, ch_emb], -1).permute(0, 2, 1, 3).reshape(B * C, T, -1)\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :T, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, -1, :].view(B, C, -1)\n",
    "        x = self.attn_norm(x + self.cross_attn(x, x, x)[0])\n",
    "        pred = self.pred_head(x).transpose(1, 2)\n",
    "        if return_features:\n",
    "            return pred, x.mean(dim=1)\n",
    "        return pred\n",
    "\n",
    "# Original (Post-LN) version for comparison\n",
    "class TransformerForecasterPostLN(nn.Module):\n",
    "    def __init__(self, n_ch, n_feat=1, h=256, n_layers=4, n_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.channel_embed = nn.Embedding(n_ch, h // 4)\n",
    "        self.input_proj = nn.Linear(n_feat + h // 4, h)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 10, h) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=h, nhead=n_heads, dim_feedforward=h * 4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.cross_attn = nn.MultiheadAttention(h, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(h)\n",
    "        self.pred_head = nn.Sequential(\n",
    "            nn.Linear(h, h), nn.GELU(), nn.Dropout(dropout), nn.Linear(h, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        B, T, C, F = x.shape\n",
    "        ch_emb = self.channel_embed(torch.arange(C, device=x.device)).unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)\n",
    "        x = torch.cat([x, ch_emb], -1).permute(0, 2, 1, 3).reshape(B * C, T, -1)\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :T, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, -1, :].view(B, C, -1)\n",
    "        x = self.attn_norm(x + self.cross_attn(x, x, x)[0])\n",
    "        pred = self.pred_head(x).transpose(1, 2)\n",
    "        if return_features:\n",
    "            return pred, x.mean(dim=1)\n",
    "        return pred\n",
    "\n",
    "def augment_batch(x, y):\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        shift = 0.15 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x.clone()\n",
    "        x[..., 0:1] = x[..., 0:1] + shift\n",
    "        y = y + shift[..., 0].squeeze(0)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        scale = 1.0 + 0.08 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x * scale\n",
    "        y = y * scale[..., 0].squeeze(0)\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        x = x + 0.03 * torch.randn_like(x)\n",
    "    return x, y\n",
    "\n",
    "for name, cls in [('Pre-LN', TransformerForecaster), ('Post-LN', TransformerForecasterPostLN)]:\n",
    "    for h, nl, nh in [(256,4,4), (256,6,4), (256,4,8), (384,4,4)]:\n",
    "        m = cls(89, 9, h, nl, nh, 0.2)\n",
    "        n = sum(p.numel() for p in m.parameters())\n",
    "        print(f'{name} h={h} L={nl} heads={nh}: {n:,} params ({n/1e6:.2f}M)')\n",
    "    print()\n",
    "print('Architecture defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Load Data\n",
    "# ============================================================================\n",
    "\n",
    "train_data = np.load(f'{TRAIN_DIR}/train_data_beignet.npz')['arr_0']\n",
    "test_public = np.load(f'{TEST_DIR}/test_data_beignet_masked.npz')['arr_0']\n",
    "\n",
    "n_features = 9\n",
    "X_train = train_data[:, :10, :, :n_features].astype(np.float32)\n",
    "Y_train = train_data[:, 10:, :, 0].astype(np.float32)\n",
    "X_target = test_public[:, :10, :, :n_features].astype(np.float32)\n",
    "\n",
    "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
    "std = X_train.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_n = (X_train - mean) / std\n",
    "Y_train_n = (Y_train - mean[...,0]) / std[...,0]\n",
    "X_target_n = (X_target - mean) / std\n",
    "\n",
    "n_val = 100\n",
    "X_tr, X_val = X_train_n[:-n_val], X_train_n[-n_val:]\n",
    "Y_tr, Y_val = Y_train_n[:-n_val], Y_train_n[-n_val:]\n",
    "\n",
    "print(f'Train: {len(X_tr)}, Val: {len(X_val)}, Target: {len(X_target_n)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Training Function (with warmup + longer training)\n",
    "# ============================================================================\n",
    "\n",
    "def train_transformer(model_cls, h, n_layers, n_heads, dropout,\n",
    "                      coral_w, mean_w, seed,\n",
    "                      epochs=400, patience=50, batch_size=32,\n",
    "                      lr=5e-4, warmup_epochs=10):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "    val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(Y_val))\n",
    "    target_ds = TensorDataset(torch.FloatTensor(X_target_n))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "    target_dl = DataLoader(target_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = model_cls(89, 9, h, n_layers, n_heads, dropout).to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Warmup + cosine schedule\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    best_val, best_state, no_improve = float('inf'), None, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        target_iter = iter(target_dl)\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            try:\n",
    "                (xt,) = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(target_dl)\n",
    "                (xt,) = next(target_iter)\n",
    "            xt = xt.to(device)\n",
    "            xb, yb = augment_batch(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, feat_src = model(xb, return_features=True)\n",
    "            _, feat_tgt = model(xt, return_features=True)\n",
    "            loss = ((pred - yb)**2).mean() + coral_w * coral_loss(feat_src, feat_tgt) + mean_w * mean_alignment_loss(feat_src, feat_tgt)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += ((model(xb) - yb)**2).sum().item()\n",
    "        val_mse = (val_loss / len(X_val)) * (std[...,0]**2).mean()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f'  seed={seed} h={h} L={n_layers} heads={n_heads} do={dropout} '\n",
    "          f'CORAL={coral_w} -> Val MSE: {best_val:.0f} ({n_params:,} params, ep {epoch+1}, {elapsed:.0f}s)')\n",
    "    return best_val, best_state\n",
    "\n",
    "print('Training function defined (warmup + cosine + augmentation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Config Sweep\n",
    "# ============================================================================\n",
    "\n",
    "configs = [\n",
    "    # (cls, h, L, heads, dropout, coral_w, mean_w, lr, name)\n",
    "    # Baseline (v206 config, but with longer training + warmup + augmentation)\n",
    "    (TransformerForecasterPostLN, 256, 4, 4, 0.20, 3.0, 1.0, 5e-4, 'postLN_h256_L4_do20'),\n",
    "    # Pre-LN variants\n",
    "    (TransformerForecaster, 256, 4, 4, 0.20, 3.0, 1.0, 5e-4, 'preLN_h256_L4_do20'),\n",
    "    (TransformerForecaster, 256, 6, 4, 0.20, 3.0, 1.0, 5e-4, 'preLN_h256_L6_do20'),\n",
    "    (TransformerForecaster, 256, 4, 8, 0.20, 3.0, 1.0, 5e-4, 'preLN_h256_L4_h8_do20'),\n",
    "    # More dropout\n",
    "    (TransformerForecaster, 256, 4, 4, 0.30, 3.0, 1.0, 5e-4, 'preLN_h256_L4_do30'),\n",
    "    # Lower LR\n",
    "    (TransformerForecaster, 256, 4, 4, 0.20, 3.0, 1.0, 3e-4, 'preLN_h256_L4_lr3e4'),\n",
    "    # Stronger CORAL\n",
    "    (TransformerForecaster, 256, 4, 4, 0.20, 5.0, 2.0, 5e-4, 'preLN_h256_L4_c5'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(f'=== Transformer Config Sweep ({len(configs)} configs) ===')\n",
    "for cls, h, nl, nh, do, cw, mw, lr, name in configs:\n",
    "    print(f'\\n--- {name} ---')\n",
    "    val_mse, state = train_transformer(cls, h, nl, nh, do, cw, mw, seed=42,\n",
    "                                       epochs=400, patience=50, lr=lr)\n",
    "    results.append((name, cls, h, nl, nh, do, cw, mw, lr, val_mse, state))\n",
    "\n",
    "results.sort(key=lambda x: x[9])\n",
    "print('\\n=== Sweep Results (sorted) ===')\n",
    "for i, (name, *_, val_mse, _) in enumerate(results):\n",
    "    tag = ' *** BEST' if i == 0 else ''\n",
    "    print(f'  {name}: Val={val_mse:,.0f}{tag}')\n",
    "\n",
    "BEST = results[0]\n",
    "print(f'\\nBest: {BEST[0]} (Val={BEST[9]:,.0f})')\n",
    "print(f'\\nFor reference, v206 TF had val ~65M (shorter training, no aug, no warmup)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Multi-seed Training with Best Config\n",
    "# ============================================================================\n",
    "\n",
    "BEST_CLS = BEST[1]\n",
    "BEST_H, BEST_NL, BEST_NH = BEST[2], BEST[3], BEST[4]\n",
    "BEST_DO, BEST_CW, BEST_MW, BEST_LR = BEST[5], BEST[6], BEST[7], BEST[8]\n",
    "\n",
    "SEEDS = [42, 123, 456, 789, 2024]\n",
    "print(f'=== Training {len(SEEDS)} seeds with best config: {BEST[0]} ===')\n",
    "\n",
    "seed_results = []\n",
    "for s in SEEDS:\n",
    "    val_mse, state = train_transformer(BEST_CLS, BEST_H, BEST_NL, BEST_NH, BEST_DO,\n",
    "                                       BEST_CW, BEST_MW, seed=s,\n",
    "                                       epochs=400, patience=50, lr=BEST_LR)\n",
    "    seed_results.append((s, val_mse, state))\n",
    "\n",
    "print(f'\\n=== Multi-seed Results ===')\n",
    "for s, val_mse, _ in seed_results:\n",
    "    print(f'  seed={s}: Val MSE = {val_mse:,.0f}')\n",
    "vals = [r[1] for r in seed_results]\n",
    "print(f'Mean: {np.mean(vals):,.0f}, Std: {np.std(vals):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Save\n",
    "# ============================================================================\n",
    "\n",
    "out_dir = f'{PROJECT_ROOT}/4_models/v213_stronger_transformer'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Check if best config uses Pre-LN or Post-LN\n",
    "is_preln = BEST_CLS == TransformerForecaster\n",
    "config = {\n",
    "    'h': BEST_H, 'n_layers': BEST_NL, 'n_heads': BEST_NH,\n",
    "    'dropout': BEST_DO, 'coral_weight': BEST_CW, 'mean_weight': BEST_MW,\n",
    "    'lr': BEST_LR, 'n_features': 9, 'pre_ln': is_preln,\n",
    "    'epochs': 400, 'patience': 50, 'warmup': 10,\n",
    "}\n",
    "print(f'Config: {config}')\n",
    "\n",
    "for s, val_mse, state in seed_results:\n",
    "    path = f'{out_dir}/model_transformer_seed{s}.pth'\n",
    "    torch.save({'model_state_dict': state, 'val_mse': val_mse, 'config': config}, path)\n",
    "    fsize = os.path.getsize(path) / (1024*1024)\n",
    "    print(f'Saved seed {s}: Val={val_mse:,.0f}, size={fsize:.1f}MB')\n",
    "\n",
    "# Save sweep results\n",
    "with open(f'{out_dir}/sweep_results.txt', 'w') as f:\n",
    "    for name, *_, val_mse, _ in results:\n",
    "        f.write(f'{name}: Val={val_mse:.0f}\\n')\n",
    "    f.write(f'\\nBest: {BEST[0]}\\n')\n",
    "    f.write(f'Pre-LN: {is_preln}\\n')\n",
    "    f.write(f'\\nMulti-seed:\\n')\n",
    "    for s, val_mse, _ in seed_results:\n",
    "        f.write(f'  seed={s}: Val={val_mse:.0f}\\n')\n",
    "\n",
    "print(f'\\nSaved to {out_dir}')\n",
    "print(f'\\nIMPORTANT: If Pre-LN wins, model.py TransformerForecaster needs norm_first=True')\n",
    "print(f'If Post-LN wins, model.py stays the same (just swap .pth files)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}