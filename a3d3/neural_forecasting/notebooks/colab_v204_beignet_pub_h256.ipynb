{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v204: Beignet Public h=256 Upgrade\n",
    "\n",
    "**\u76ee\u7684:** \u5347\u7d1a Beignet Public TCN \u5f9e h=128 \u5230 h=256\uff0c\u5927\u5e45\u964d\u4f4e Beignet Public MSE\n",
    "\n",
    "**\u5df2\u77e5\u6700\u4f73 h=128 \u914d\u7f6e:**\n",
    "- h=128, L=3, dropout=0.25, CORAL \u03bb=3.0, Mean=1.0, 9 features -> Public MSE 52,323\n",
    "\n",
    "**Sweep \u8a08\u756b:**\n",
    "- h=256 with different L (3, 4), dropout (0.25, 0.35), CORAL \u03bb (3.0, 5.0, 7.0)\n",
    "- \u627e\u5230\u6700\u4f73\u914d\u7f6e\u5f8c\u8a13\u7df4 5 seeds\n",
    "\n",
    "**\u63a8\u8ad6\u6642\u9593\u9810\u7b97:** 5 seeds h=256 ~ +86s on Codabench -> total ~356s / 600s = SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 1: Setup\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/Hackathon_NSF_Neural_Forecasting'\n",
    "TRAIN_DIR = f'{PROJECT_ROOT}/1_data/raw/train_data_neuro'\n",
    "TEST_DIR = f'{PROJECT_ROOT}/1_data/raw/test_dev_input'\n",
    "\n",
    "import os, time, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Architecture (CORAL losses + TCN classes + augmentation)\n",
    "# ============================================================================\n",
    "\n",
    "# --- CORAL Losses ---\n",
    "\n",
    "def coral_loss(source, target):\n",
    "    d = source.size(1)\n",
    "    cs = (source - source.mean(0, keepdim=True)).T @ (source - source.mean(0, keepdim=True)) / (source.size(0) - 1 + 1e-8)\n",
    "    ct = (target - target.mean(0, keepdim=True)).T @ (target - target.mean(0, keepdim=True)) / (target.size(0) - 1 + 1e-8)\n",
    "    return ((cs - ct) ** 2).sum() / (4 * d)\n",
    "\n",
    "def mean_alignment_loss(source, target):\n",
    "    return ((source.mean(0) - target.mean(0)) ** 2).mean()\n",
    "\n",
    "# --- TCN Architecture ---\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation)\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out[:, :, :-self.padding] if self.padding > 0 else out\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = CausalConv1d(in_ch, out_ch, kernel_size, dilation)\n",
    "        self.conv2 = CausalConv1d(out_ch, out_ch, kernel_size, dilation)\n",
    "        self.norm1, self.norm2 = nn.BatchNorm1d(out_ch), nn.BatchNorm1d(out_ch)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        r = self.residual(x)\n",
    "        x = self.dropout(self.activation(self.norm1(self.conv1(x))))\n",
    "        x = self.dropout(self.activation(self.norm2(self.conv2(x))))\n",
    "        return x + r\n",
    "\n",
    "class TCNEncoder(nn.Module):\n",
    "    def __init__(self, in_size, h_size, n_layers=4, k_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(in_size, h_size, 1)\n",
    "        self.layers = nn.ModuleList([TCNBlock(h_size, h_size, k_size, 2**i, dropout) for i in range(n_layers)])\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x.transpose(1,2))\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x.transpose(1,2)\n",
    "\n",
    "class TCNForecaster(nn.Module):\n",
    "    def __init__(self, n_ch, n_feat=1, h=64, n_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.channel_embed = nn.Embedding(n_ch, h//4)\n",
    "        self.input_proj = nn.Linear(n_feat + h//4, h)\n",
    "        self.tcn = TCNEncoder(h, h, n_layers, 3, dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(h, 4, dropout=dropout, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(h)\n",
    "        self.pred_head = nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.Dropout(dropout), nn.Linear(h,10))\n",
    "    def forward(self, x, return_features=False):\n",
    "        B,T,C,F = x.shape\n",
    "        ch_emb = self.channel_embed(torch.arange(C, device=x.device)).unsqueeze(0).unsqueeze(0).expand(B,T,-1,-1)\n",
    "        x = torch.cat([x, ch_emb], -1).permute(0,2,1,3).reshape(B*C,T,-1)\n",
    "        x = self.tcn(self.input_proj(x))\n",
    "        x = x[:,-1,:].view(B,C,-1)\n",
    "        x = self.attn_norm(x + self.cross_attn(x,x,x)[0])\n",
    "        pred = self.pred_head(x).transpose(1,2)\n",
    "        if return_features:\n",
    "            return pred, x.mean(dim=1)\n",
    "        return pred\n",
    "\n",
    "# --- Augmentation ---\n",
    "\n",
    "def augment_batch(x, y):\n",
    "    \"\"\"Apply augmentations to source batch during training.\n",
    "    x: (B, T, C, F) normalized\n",
    "    y: (B, T_out, C) normalized\n",
    "    \"\"\"\n",
    "    # 1. Channel-wise mean shift (simulates baseline drift between domains)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        shift = 0.15 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x.clone()\n",
    "        x[..., 0:1] = x[..., 0:1] + shift\n",
    "        y = y + shift[..., 0].squeeze(0)\n",
    "    # 2. Amplitude scaling per channel (simulates gain drift)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        scale = 1.0 + 0.08 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x * scale\n",
    "        y = y * scale[..., 0].squeeze(0)\n",
    "    # 3. Gaussian noise (regularization)\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        x = x + 0.03 * torch.randn_like(x)\n",
    "    return x, y\n",
    "\n",
    "# --- Param count check ---\n",
    "for h, nl in [(128, 3), (256, 3), (256, 4)]:\n",
    "    m = TCNForecaster(89, 9, h, nl, 0.25)\n",
    "    n = sum(p.numel() for p in m.parameters())\n",
    "    print(f'h={h}, layers={nl}: {n:,} params ({n/1e6:.2f}M)')\n",
    "\n",
    "print('\\nArchitecture, CORAL losses, and augmentation defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Load Data\n",
    "# ============================================================================\n",
    "\n",
    "train_data = np.load(f'{TRAIN_DIR}/train_data_beignet.npz')['arr_0']\n",
    "test_public = np.load(f'{TEST_DIR}/test_data_beignet_masked.npz')['arr_0']\n",
    "\n",
    "n_features = 9\n",
    "X_train = train_data[:, :10, :, :n_features].astype(np.float32)\n",
    "Y_train = train_data[:, 10:, :, 0].astype(np.float32)\n",
    "X_target = test_public[:, :10, :, :n_features].astype(np.float32)\n",
    "\n",
    "# Normalization: mean/std computed from X_train over axes (0,1)\n",
    "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
    "std = X_train.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_n = (X_train - mean) / std\n",
    "Y_train_n = (Y_train - mean[...,0]) / std[...,0]\n",
    "X_target_n = (X_target - mean) / std\n",
    "\n",
    "# Validation split: last 100 samples\n",
    "n_val = 100\n",
    "X_tr, X_val = X_train_n[:-n_val], X_train_n[-n_val:]\n",
    "Y_tr, Y_val = Y_train_n[:-n_val], Y_train_n[-n_val:]\n",
    "\n",
    "print(f'Train: {len(X_tr)}, Val: {len(X_val)}, Target: {len(X_target_n)}')\n",
    "print(f'X shape: {X_tr.shape}, Y shape: {Y_tr.shape}')\n",
    "print(f'Mean shape: {mean.shape}, Std shape: {std.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_coral_model(h_size, n_layers, dropout, coral_w, mean_w, seed,\n",
    "                      epochs=250, patience=30, batch_size=32, use_aug=True):\n",
    "    \"\"\"Train a single TCN CORAL model with given config and seed.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "    val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(Y_val))\n",
    "    target_ds = TensorDataset(torch.FloatTensor(X_target_n))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "    target_dl = DataLoader(target_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = TCNForecaster(89, 9, h_size, n_layers, dropout).to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_val, best_state, no_improve = float('inf'), None, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        target_iter = iter(target_dl)\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            try:\n",
    "                (xt,) = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(target_dl)\n",
    "                (xt,) = next(target_iter)\n",
    "            xt = xt.to(device)\n",
    "\n",
    "            # Apply augmentation\n",
    "            if use_aug:\n",
    "                xb, yb = augment_batch(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred, feat_src = model(xb, return_features=True)\n",
    "            _, feat_tgt = model(xt, return_features=True)\n",
    "            loss = ((pred - yb)**2).mean() + coral_w * coral_loss(feat_src, feat_tgt) + mean_w * mean_alignment_loss(feat_src, feat_tgt)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                val_loss += ((model(xb) - yb)**2).sum().item()\n",
    "        val_mse = (val_loss / len(X_val)) * (std[...,0]**2).mean()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f'  seed={seed} h={h_size} L={n_layers} do={dropout} CORAL={coral_w} Mean={mean_w} '\n",
    "          f'-> Val MSE: {best_val:.0f} ({n_params:,} params, ep {epoch+1}, {elapsed:.0f}s)')\n",
    "    return best_val, best_state\n",
    "\n",
    "print('Training function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Architecture Sweep\n",
    "# ============================================================================\n",
    "\n",
    "configs = [\n",
    "    # (h, L, dropout, coral_lambda, mean_lambda, name)\n",
    "    (256, 3, 0.25, 3.0, 1.0, \"h256_L3_do025_c3\"),     # baseline scaled up\n",
    "    (256, 3, 0.35, 3.0, 1.0, \"h256_L3_do035_c3\"),     # more dropout\n",
    "    (256, 4, 0.30, 3.0, 1.0, \"h256_L4_do030_c3\"),     # deeper\n",
    "    (256, 3, 0.25, 5.0, 2.0, \"h256_L3_do025_c5\"),     # stronger CORAL\n",
    "    (256, 3, 0.30, 5.0, 2.0, \"h256_L3_do030_c5\"),     # stronger CORAL + dropout\n",
    "    (256, 3, 0.25, 7.0, 3.0, \"h256_L3_do025_c7\"),     # aggressive CORAL\n",
    "    (128, 3, 0.25, 3.0, 1.0, \"h128_baseline\"),         # baseline for comparison\n",
    "]\n",
    "\n",
    "# Run each with seed=42\n",
    "results = []\n",
    "for h, nl, do, cw, mw, name in configs:\n",
    "    print(f'\\n=== {name} ===')\n",
    "    val_mse, state = train_coral_model(h, nl, do, cw, mw, seed=42)\n",
    "    results.append((name, h, nl, do, cw, mw, val_mse, state))\n",
    "\n",
    "# Sort and print\n",
    "results.sort(key=lambda x: x[6])\n",
    "print('\\n=== Sweep Results (sorted by val_mse) ===')\n",
    "for name, h, nl, do, cw, mw, val_mse, _ in results:\n",
    "    print(f'  {name}: Val MSE = {val_mse:,.0f}')\n",
    "best = results[0]\n",
    "print(f'\\nBest: {best[0]} (Val MSE: {best[6]:,.0f})')\n",
    "BEST_H, BEST_NL, BEST_DO, BEST_CW, BEST_MW = best[1], best[2], best[3], best[4], best[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Multi-seed Training with Best Config\n",
    "# ============================================================================\n",
    "\n",
    "SEEDS = [42, 123, 456, 789, 2024]\n",
    "print(f'=== Training {len(SEEDS)} seeds with best config: h={BEST_H} L={BEST_NL} do={BEST_DO} CORAL={BEST_CW} Mean={BEST_MW} ===')\n",
    "\n",
    "seed_results = []\n",
    "for s in SEEDS:\n",
    "    val_mse, state = train_coral_model(BEST_H, BEST_NL, BEST_DO, BEST_CW, BEST_MW, seed=s)\n",
    "    seed_results.append((s, val_mse, state))\n",
    "\n",
    "print(f'\\n=== Multi-seed Results ===')\n",
    "for s, val_mse, _ in seed_results:\n",
    "    print(f'  seed={s}: Val MSE = {val_mse:,.0f}')\n",
    "val_mses = [r[1] for r in seed_results]\n",
    "print(f'Mean: {np.mean(val_mses):,.0f}, Std: {np.std(val_mses):,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Save Checkpoints\n",
    "# ============================================================================\n",
    "\n",
    "out_dir = f'{PROJECT_ROOT}/4_models/v204_beignet_pub_h256'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'h_size': BEST_H,\n",
    "    'n_layers': BEST_NL,\n",
    "    'dropout': BEST_DO,\n",
    "    'coral_weight': BEST_CW,\n",
    "    'mean_weight': BEST_MW,\n",
    "    'n_features': 9,\n",
    "    'seeds': SEEDS,\n",
    "}\n",
    "print(f'Config: {config}')\n",
    "\n",
    "for s, val_mse, state in seed_results:\n",
    "    path = f'{out_dir}/model_tcn_seed{s}.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': state,\n",
    "        'val_mse': val_mse,\n",
    "        'config': config,\n",
    "    }, path)\n",
    "    fsize = os.path.getsize(path) / 1024\n",
    "    print(f'Saved seed {s}: Val={val_mse:,.0f}, size={fsize:.0f}KB')\n",
    "\n",
    "# Also save sweep results\n",
    "sweep_path = f'{out_dir}/sweep_results.txt'\n",
    "with open(sweep_path, 'w') as f:\n",
    "    for name, h, nl, do, cw, mw, val_mse, _ in results:\n",
    "        f.write(f'{name}: h={h} L={nl} do={do} CORAL={cw} Mean={mw} Val={val_mse:.0f}\\n')\n",
    "print(f'Sweep results saved to {sweep_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Download\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import files\n",
    "for s in SEEDS:\n",
    "    files.download(f'{out_dir}/model_tcn_seed{s}.pth')\n",
    "\n",
    "print('\\n=== Done! ===')\n",
    "print(f'Best config: h={BEST_H}, L={BEST_NL}, do={BEST_DO}, CORAL={BEST_CW}, Mean={BEST_MW}')\n",
    "print(f'Seeds: {SEEDS}')\n",
    "print(f'\\nFor model.py, update config:')\n",
    "print(f'  V200_H = {BEST_H}')\n",
    "print(f'  V200_LAYERS = {BEST_NL}')\n",
    "print(f'  V200_DROPOUT = {BEST_DO}')\n",
    "print(f'  V200_SEEDS = {SEEDS}')"
   ]
  }
 ]
}