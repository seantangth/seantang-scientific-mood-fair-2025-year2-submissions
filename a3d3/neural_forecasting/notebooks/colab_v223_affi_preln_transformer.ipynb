{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v223: Affi Pre-LN Transformer with CORAL\n",
    "\n",
    "- Architecture: h=384, L=4, heads=8, Pre-LN (norm_first=True)\n",
    "- Training: CORAL + Mean alignment, mixed precision\n",
    "- Goal: Add TF diversity to Affi (currently TCN-only)\n",
    "- Beignet saw -1,411 improvement from adding TF to ensemble\n",
    "- Train both CORAL and non-CORAL versions (for 4-model blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\nPROJECT_ROOT = '/content/drive/MyDrive/Hackathon_NSF_Neural_Forecasting'\nTRAIN_DIR = f'{PROJECT_ROOT}/1_data/raw/train_data_neuro'\nTEST_DIR = f'{PROJECT_ROOT}/1_data/raw/test_dev_input'\n\nimport os, time, torch, numpy as np, math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.cuda.amp import autocast, GradScaler\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Losses + Architecture + Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "def coral_loss(source, target):\n",
    "    d = source.size(1)\n",
    "    cs = (source - source.mean(0, keepdim=True)).T @ (source - source.mean(0, keepdim=True)) / (source.size(0) - 1 + 1e-8)\n",
    "    ct = (target - target.mean(0, keepdim=True)).T @ (target - target.mean(0, keepdim=True)) / (target.size(0) - 1 + 1e-8)\n",
    "    return ((cs - ct) ** 2).sum() / (4 * d)\n",
    "\n",
    "def mean_alignment_loss(source, target):\n",
    "    return ((source.mean(0) - target.mean(0)) ** 2).mean()\n",
    "\n",
    "class TransformerForecasterLarge(nn.Module):\n",
    "    \"\"\"Pre-LN Transformer for Affi (239 channels).\"\"\"\n",
    "    def __init__(self, n_ch, n_feat=1, h=384, n_layers=4, n_heads=8, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.channel_embed = nn.Embedding(n_ch, h // 4)\n",
    "        self.input_proj = nn.Linear(n_feat + h // 4, h)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 10, h) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=h, nhead=n_heads, dim_feedforward=h * 4,\n",
    "            dropout=dropout, batch_first=True, activation='gelu',\n",
    "            norm_first=True  # Pre-LN\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.cross_attn = nn.MultiheadAttention(h, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(h)\n",
    "        self.pred_head = nn.Sequential(\n",
    "            nn.Linear(h, h), nn.GELU(), nn.Dropout(dropout), nn.Linear(h, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        B, T, C, F = x.shape\n",
    "        ch_emb = self.channel_embed(torch.arange(C, device=x.device)).unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)\n",
    "        x = torch.cat([x, ch_emb], -1).permute(0, 2, 1, 3).reshape(B * C, T, -1)\n",
    "        x = self.input_proj(x) + self.pos_embed[:, :T, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, -1, :].view(B, C, -1)\n",
    "        x = self.attn_norm(x + self.cross_attn(x, x, x)[0])\n",
    "        pred = self.pred_head(x).transpose(1, 2)\n",
    "        if return_features:\n",
    "            return pred, x.mean(dim=1)\n",
    "        return pred\n",
    "\n",
    "def augment_batch(x, y):\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        shift = 0.15 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x.clone()\n",
    "        x[..., 0:1] = x[..., 0:1] + shift\n",
    "        y = y + shift[..., 0].squeeze(0)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        scale = 1.0 + 0.08 * torch.randn(1, 1, x.shape[2], 1, device=x.device)\n",
    "        x = x * scale\n",
    "        y = y * scale[..., 0].squeeze(0)\n",
    "    if torch.rand(1).item() < 0.3:\n",
    "        x = x + 0.03 * torch.randn_like(x)\n",
    "    return x, y\n",
    "\n",
    "# Print param count\n",
    "m = TransformerForecasterLarge(239, 3, 384, 4, 8, 0.2)\n",
    "n_params = sum(p.numel() for p in m.parameters())\n",
    "print(f'TransformerForecasterLarge(239ch, 3feat, h=384, L=4, heads=8): {n_params:,} params ({n_params/1e6:.2f}M)')\n",
    "del m\n",
    "print('Architecture defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: Load Data\n",
    "# ============================================================================\n",
    "\n",
    "# Load Affi data - BOTH 3-feat and 1-feat versions\n",
    "train_data = np.load(f'{TRAIN_DIR}/train_data_affi.npz')['arr_0']\n",
    "test_public = np.load(f'{TEST_DIR}/test_data_affi_masked.npz')['arr_0']\n",
    "\n",
    "# 3-feat normalization\n",
    "N_FEAT_3 = 3\n",
    "X_train_3 = train_data[:, :10, :, :N_FEAT_3].astype(np.float32)\n",
    "Y_train = train_data[:, 10:, :, 0].astype(np.float32)\n",
    "X_target_3 = test_public[:, :10, :, :N_FEAT_3].astype(np.float32)\n",
    "\n",
    "mean_3 = X_train_3.mean(axis=(0,1), keepdims=True)\n",
    "std_3 = X_train_3.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_3n = (X_train_3 - mean_3) / std_3\n",
    "Y_train_n = (Y_train - mean_3[...,0]) / std_3[...,0]\n",
    "X_target_3n = (X_target_3 - mean_3) / std_3\n",
    "\n",
    "n_val = 100\n",
    "X_tr_3, X_val_3 = X_train_3n[:-n_val], X_train_3n[-n_val:]\n",
    "Y_tr, Y_val = Y_train_n[:-n_val], Y_train_n[-n_val:]\n",
    "\n",
    "print(f'Train: {len(X_tr_3)}, Val: {len(X_val_3)}, Target: {len(X_target_3n)}')\n",
    "print(f'Channels: {X_train_3.shape[2]}, Features: {N_FEAT_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_affi_transformer(n_feat, X_tr, X_val, Y_tr, Y_val, X_target,\n",
    "                           mean, std, h=384, n_layers=4, n_heads=8, dropout=0.2,\n",
    "                           coral_w=3.0, mean_w=1.0, seed=42,\n",
    "                           epochs=200, patience=30, batch_size=16,\n",
    "                           lr=5e-4, warmup_epochs=10):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed(seed)\n",
    "\n",
    "    train_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(Y_tr))\n",
    "    val_ds = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(Y_val))\n",
    "    target_ds = TensorDataset(torch.FloatTensor(X_target))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "    target_dl = DataLoader(target_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = TransformerForecasterLarge(239, n_feat, h, n_layers, n_heads, dropout).to(device)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Warmup + cosine schedule\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    best_val, best_state, no_improve = float('inf'), None, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        target_iter = iter(target_dl)\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            try: (xt,) = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(target_dl)\n",
    "                (xt,) = next(target_iter)\n",
    "            xt = xt.to(device)\n",
    "            xb, yb = augment_batch(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                pred, feat_src = model(xb, return_features=True)\n",
    "                _, feat_tgt = model(xt, return_features=True)\n",
    "                mse_loss = ((pred - yb)**2).mean()\n",
    "                if coral_w > 0:\n",
    "                    c_loss = coral_loss(feat_src.float(), feat_tgt.float())\n",
    "                    m_loss = mean_alignment_loss(feat_src.float(), feat_tgt.float())\n",
    "                    loss = mse_loss + coral_w * c_loss + mean_w * m_loss\n",
    "                else:\n",
    "                    loss = mse_loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with autocast():\n",
    "                    val_loss += ((model(xb) - yb)**2).sum().item()\n",
    "        val_mse = (val_loss / len(X_val)) * (std[...,0]**2).mean()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if no_improve >= patience: break\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    tag = 'CORAL' if coral_w > 0 else 'orig'\n",
    "    print(f'  [{tag}] seed={seed} feat={n_feat} h={h} L={n_layers} heads={n_heads} '\n",
    "          f'-> Val MSE: {best_val:,.0f} ({n_params:,} params, ep {epoch+1}, {elapsed:.0f}s)')\n",
    "    del model; torch.cuda.empty_cache()\n",
    "    return best_val, best_state\n",
    "\n",
    "print('Training function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Train 3-feat CORAL + orig (5 seeds each)\n",
    "# ============================================================================\n",
    "\n",
    "SEEDS = [42, 123, 456, 789, 2024]\n",
    "\n",
    "print('=== Training 3-feat CORAL (5 seeds) ===')\n",
    "coral_3f_results = []\n",
    "for s in SEEDS:\n",
    "    val_mse, state = train_affi_transformer(\n",
    "        3, X_tr_3, X_val_3, Y_tr, Y_val, X_target_3n,\n",
    "        mean_3, std_3, h=384, n_layers=4, n_heads=8, dropout=0.2,\n",
    "        coral_w=3.0, mean_w=1.0, seed=s, epochs=200, patience=30)\n",
    "    coral_3f_results.append((s, val_mse, state))\n",
    "\n",
    "print('\\n=== Training 3-feat orig/no-CORAL (5 seeds) ===')\n",
    "orig_3f_results = []\n",
    "for s in SEEDS:\n",
    "    val_mse, state = train_affi_transformer(\n",
    "        3, X_tr_3, X_val_3, Y_tr, Y_val, X_target_3n,\n",
    "        mean_3, std_3, h=384, n_layers=4, n_heads=8, dropout=0.2,\n",
    "        coral_w=0.0, mean_w=0.0, seed=s, epochs=200, patience=30)\n",
    "    orig_3f_results.append((s, val_mse, state))\n",
    "\n",
    "print('\\n=== 3-feat Results ===')\n",
    "for s, v, _ in coral_3f_results:\n",
    "    print(f'  CORAL seed={s}: Val={v:,.0f}')\n",
    "for s, v, _ in orig_3f_results:\n",
    "    print(f'  orig  seed={s}: Val={v:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Save models\n",
    "# ============================================================================\n",
    "\n",
    "out_dir = f'{PROJECT_ROOT}/4_models/v223_affi_preln_transformer'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'n_ch': 239, 'n_feat': 3, 'h': 384, 'n_layers': 4, 'n_heads': 8,\n",
    "    'dropout': 0.2, 'pre_ln': True, 'coral_weight': 3.0, 'mean_weight': 1.0,\n",
    "}\n",
    "\n",
    "for s, val_mse, state in coral_3f_results:\n",
    "    path = f'{out_dir}/model_affi_tf_3feat_coral_seed{s}.pth'\n",
    "    torch.save({'model_state_dict': state, 'val_mse': val_mse, 'config': config}, path)\n",
    "    fsize = os.path.getsize(path) / (1024*1024)\n",
    "    print(f'CORAL seed {s}: Val={val_mse:,.0f}, size={fsize:.1f}MB')\n",
    "\n",
    "config_orig = dict(config); config_orig['coral_weight'] = 0.0; config_orig['mean_weight'] = 0.0\n",
    "for s, val_mse, state in orig_3f_results:\n",
    "    path = f'{out_dir}/model_affi_tf_3feat_orig_seed{s}.pth'\n",
    "    torch.save({'model_state_dict': state, 'val_mse': val_mse, 'config': config_orig}, path)\n",
    "    fsize = os.path.getsize(path) / (1024*1024)\n",
    "    print(f'orig  seed {s}: Val={val_mse:,.0f}, size={fsize:.1f}MB')\n",
    "\n",
    "# Save normalization\n",
    "np.savez(f'{out_dir}/normalization_affi_3feat.npz', mean=mean_3, std=std_3)\n",
    "\n",
    "# Save results summary\n",
    "with open(f'{out_dir}/results.txt', 'w') as f:\n",
    "    f.write('v223 Affi Pre-LN Transformer Results\\n')\n",
    "    f.write(f'Config: {config}\\n\\n')\n",
    "    f.write('3-feat CORAL:\\n')\n",
    "    for s, v, _ in coral_3f_results:\n",
    "        f.write(f'  seed={s}: Val={v:.0f}\\n')\n",
    "    f.write('\\n3-feat orig:\\n')\n",
    "    for s, v, _ in orig_3f_results:\n",
    "        f.write(f'  seed={s}: Val={v:.0f}\\n')\n",
    "\n",
    "print(f'\\nSaved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Download best models\n",
    "# ============================================================================\n",
    "\n",
    "# Sort by val MSE, pick best 3 for each\n",
    "coral_sorted = sorted(coral_3f_results, key=lambda x: x[1])\n",
    "orig_sorted = sorted(orig_3f_results, key=lambda x: x[1])\n",
    "\n",
    "print('=== Best 3 CORAL ===')\n",
    "for s, v, _ in coral_sorted[:3]:\n",
    "    print(f'  seed={s}: Val={v:,.0f}')\n",
    "\n",
    "print('\\n=== Best 3 orig ===')\n",
    "for s, v, _ in orig_sorted[:3]:\n",
    "    print(f'  seed={s}: Val={v:,.0f}')\n",
    "\n",
    "print(f'\\nAll models saved to: {out_dir}')\n",
    "print(f'\\nFor model.py integration:')\n",
    "print(f'  - Add TransformerForecasterLarge class (Pre-LN version)')\n",
    "print(f'  - Load TF models alongside existing TCN models')\n",
    "print(f'  - Ensemble: alpha * TCN_4model_blend + (1-alpha) * TF_ensemble')\n",
    "print(f'  - Optimize alpha on val data')"
   ]
  }
 ]
}