{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# V123: MLP Head LP-FT KFold\n",
    "\n",
    "**V121 的問題**: Linear head 是瓶頸 (R²=0.4099)。MLP 可以捕捉非線性特徵組合。\n",
    "\n",
    "**Strategy**: 2-stage，但 Stage 1 改為 MLP 訓練（而非 Ridge）\n",
    "1. **Stage 1 (LP)**: 凍結 BioCLIP backbone → 只訓練 MLP head（用快取特徵，快）\n",
    "2. **Stage 2 (FT)**: 載入 Stage 1 MLP 權重 → 解凍 backbone → fine-tune (backbone LR=1e-6)\n",
    "\n",
    "**MLP Head**: `Linear(768,256) → BN → ReLU → Dropout(0.1) → Linear(256,3)`\n",
    "\n",
    "**與 V121 的差異**:\n",
    "- Stage 1 用 MLP on cached features（非 Ridge）\n",
    "- Head 有非線性層\n",
    "- 共用 V121 的 features_cache.pkl（省 5 分鐘）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "!pip install -q open_clip_torch datasets transformers scikit-learn\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PROJECT  = '/content/drive/MyDrive/Hackathon_NSF_Beetles'\n",
    "SAVE_DIR = f'{PROJECT}/4_models/v123_mlp_lpft_kfold'\n",
    "V121_DIR = f'{PROJECT}/4_models/v121_lpft_kfold'  # reuse feature cache\n",
    "\n",
    "import os, json, time, pickle, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "from datasets import load_dataset\n",
    "import open_clip\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f'Save dir: {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load dataset + BioCLIP preprocess\n",
    "print('Loading sentinel-beetles dataset...')\n",
    "ds = load_dataset('imageomics/sentinel-beetles')\n",
    "print(f'Train: {len(ds[\"train\"])}, Val: {len(ds[\"validation\"])}')\n",
    "\n",
    "_, _, bioclip_preprocess = open_clip.create_model_and_transforms(\n",
    "    'hf-hub:imageomics/bioclip-2', output_dict=True, require_pretrained=True)\n",
    "\n",
    "train_augmentation = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "])\n",
    "\n",
    "TARGETS = ['SPEI_30d', 'SPEI_1y', 'SPEI_2y']\n",
    "\n",
    "class SimpleConcat:\n",
    "    def __init__(self, ds1, ds2):\n",
    "        self.ds1, self.ds2, self.len1 = ds1, ds2, len(ds1)\n",
    "    def __len__(self): return self.len1 + len(self.ds2)\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)\n",
    "        return self.ds1[idx] if idx < self.len1 else self.ds2[idx - self.len1]\n",
    "\n",
    "full_ds = SimpleConcat(ds['train'], ds['validation'])\n",
    "N = len(full_ds)\n",
    "print(f'Total samples: {N}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Load frozen features\n# 優先使用 V121 的快取（同一個 backbone，完全相容）\nV121_CACHE = f'{V121_DIR}/features_cache.pkl'\nV123_CACHE = f'{SAVE_DIR}/features_cache.pkl'\n\nif os.path.exists(V121_CACHE):\n    print(f'Reusing V121 cache: {V121_CACHE}')\n    cache_path = V121_CACHE\nelif os.path.exists(V123_CACHE):\n    print(f'Using V123 cache: {V123_CACHE}')\n    cache_path = V123_CACHE\nelse:\n    print('No cache found → extracting features (~5 min)...')\n    bioclip_model, _, _ = open_clip.create_model_and_transforms(\n        'hf-hub:imageomics/bioclip-2', output_dict=True, require_pretrained=True)\n    bioclip_model = bioclip_model.to(device).eval()\n\n    all_features, all_labels = [], []\n    batch_size = 64\n    for start in range(0, N, batch_size):\n        end = min(start + batch_size, N)\n        images, labels = [], []\n        for i in range(start, end):\n            row = full_ds[i]\n            images.append(bioclip_preprocess(row['file_path'].convert('RGB')))\n            labels.append([row[t] for t in TARGETS])\n        images_t = torch.stack(images).to(device)\n        with torch.no_grad(), autocast():\n            feat = bioclip_model(images_t)['image_features']\n        all_features.append(feat.float().cpu().numpy())\n        all_labels.append(np.array(labels))\n        if (start // batch_size) % 50 == 0:\n            print(f'  {start}/{N} ({100*start/N:.0f}%)')\n\n    all_features = np.concatenate(all_features)\n    all_labels   = np.concatenate(all_labels)\n    with open(V123_CACHE, 'wb') as f:\n        pickle.dump({'features': all_features, 'labels': all_labels}, f)\n    cache_path = V123_CACHE\n    del bioclip_model; torch.cuda.empty_cache()\n\nif cache_path != V123_CACHE or 'all_features' not in dir():\n    with open(cache_path, 'rb') as f:\n        cache = pickle.load(f)\n    all_features = cache['features']\n    all_labels   = cache['labels']\n\n# === Alignment Safety Guard ===\n# 確保快取樣本數與當前 dataset 完全一致，避免 OOF fold 錯位\nassert all_features.shape[0] == N, (\n    f'Cache size mismatch! cache={all_features.shape[0]}, dataset={N}. '\n    f'Delete {cache_path} and re-extract.'\n)\nassert all_features.shape[1] == 768, f'Feature dim mismatch: {all_features.shape[1]} != 768'\nassert all_labels.shape == (N, 3), f'Label shape mismatch: {all_labels.shape}'\nprint(f'Alignment check passed: {N} samples, features {all_features.shape}')\n\nprint(f'Feature std: {all_features.std():.4f}')"
  },
  {
   "cell_type": "code",
   "id": "fi09893az89",
   "source": "# Cell 3b: Pre-cache images as uint8 tensors → 消除 Stage 2 的 GPU idle from I/O\n#\n# 只做 Resize(224, BICUBIC) + CenterCrop(224) → uint8 (3,224,224)\n# Augmentation (RandomFlip + ColorJitter) 和 Normalize 仍在 __getitem__ on-the-fly 做\n# 2000 張圖約 300MB RAM，Colab 完全吃得下\n#\nIMG_CACHE_PATH = f'{SAVE_DIR}/image_cache.pt'\n\n_resize_crop = transforms.Compose([\n    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n    transforms.CenterCrop(224),\n    transforms.PILToTensor(),  # → uint8 (3, 224, 224)\n])\n\nif os.path.exists(IMG_CACHE_PATH):\n    print(f'Loading image cache from Drive...')\n    t0 = time.time()\n    image_cache = torch.load(IMG_CACHE_PATH, map_location='cpu')\n    print(f'Loaded: {image_cache.shape} {image_cache.dtype}  '\n          f'{image_cache.element_size() * image_cache.nelement() / 1e6:.0f} MB  '\n          f'({time.time()-t0:.1f}s)')\nelse:\n    print(f'Building image cache ({N} images)...')\n    t0 = time.time()\n    image_cache = torch.zeros((N, 3, 224, 224), dtype=torch.uint8)\n    for i in range(N):\n        row = full_ds[i]\n        image_cache[i] = _resize_crop(row['file_path'].convert('RGB'))\n        if i % 500 == 0:\n            print(f'  {i}/{N}  ({time.time()-t0:.0f}s)')\n    torch.save(image_cache, IMG_CACHE_PATH)\n    mb = image_cache.element_size() * image_cache.nelement() / 1e6\n    print(f'Done: {mb:.0f} MB → {IMG_CACHE_PATH}  ({time.time()-t0:.0f}s)')\n\n# Alignment check：確保和 all_features / all_labels 一一對應\nassert image_cache.shape == (N, 3, 224, 224), \\\n    f'Image cache shape mismatch: {image_cache.shape} vs expected ({N}, 3, 224, 224)'\nprint('Image cache ready ✓')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Config\nCONFIG = {\n    'n_folds': 5,\n    'seed': 42,\n    # Stage 1: MLP on frozen features\n    'lp_hidden': 256,\n    'lp_dropout': 0.1,\n    'lp_epochs': 150,\n    'lp_patience': 20,\n    'lp_batch_size': 512,\n    'lp_lr': 1e-3,\n    'lp_weight_decay': 1e-4,\n    # Stage 2: Fine-tune backbone + head\n    # ft_batch_size=64: A100 效率（~25 steps/epoch，warmup 能正常運作）\n    # ft_lr_head=1e-4:  與 V121 linear head 相同（backbone 1e-6 的 100x）\n    # num_workers=4:    A100 I/O 充分餵飽 GPU\n    'ft_epochs': 12,\n    'ft_patience': 5,\n    'ft_batch_size': 64,\n    'ft_lr_backbone': 1e-6,\n    'ft_lr_head': 1e-4,\n    'ft_weight_decay': 0.01,\n    'ft_warmup_epochs': 1,\n    'ft_num_workers': 4,\n}\n\nwith open(f'{SAVE_DIR}/config.json', 'w') as f:\n    json.dump(CONFIG, f, indent=2)\nprint('Config:', CONFIG)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Model definitions\n\ndef build_mlp_head(in_dim=768, hidden=256, dropout=0.1, out_dim=3):\n    \"\"\"Linear(768,256) → BN → ReLU → Dropout → Linear(256,3)\"\"\"\n    return nn.Sequential(\n        nn.Linear(in_dim, hidden),\n        nn.BatchNorm1d(hidden),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden, out_dim),\n    )\n\n\nclass BioCLIP_MLP_LPFT(nn.Module):\n    \"\"\"BioCLIP + MLP head，支援 Stage 2 fine-tuning\"\"\"\n    def __init__(self, hidden=256, dropout=0.1, head_state=None):\n        super().__init__()\n        self.bioclip, _, _ = open_clip.create_model_and_transforms(\n            'hf-hub:imageomics/bioclip-2', output_dict=True, require_pretrained=True)\n\n        # Probe output dim\n        with torch.no_grad():\n            dummy = torch.randn(1, 3, 224, 224)\n            out_dim = self.bioclip(dummy)['image_features'].shape[-1]\n\n        self.head = build_mlp_head(out_dim, hidden, dropout)\n\n        if head_state is not None:\n            self.head.load_state_dict(head_state)\n            print(f'  Stage 1 MLP weights loaded into head.')\n        else:\n            print(f'  Random head init: {out_dim} → {hidden} → 3')\n\n    def forward(self, x):\n        features = self.bioclip(x)['image_features']\n        return self.head(features)\n\n\nclass BeetleImageDataset(torch.utils.data.Dataset):\n    \"\"\"從 RAM 內的 uint8 image_cache 讀取，消除 Stage 2 磁碟 I/O。\n    \n    uint8 → float32 → (augment) → normalize 全在 __getitem__ 做，\n    避免在快取時就做 normalize（節省 RAM 3×）。\n    \"\"\"\n    _normalize = transforms.Normalize(\n        mean=(0.48145466, 0.4578275, 0.40821073),\n        std=(0.26862954, 0.26130258, 0.27577711))\n    _aug = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n    ])\n\n    def __init__(self, image_cache, labels, indices, is_train=False):\n        # image_cache: (N, 3, 224, 224) uint8 CPU tensor\n        # labels:      (N, 3) numpy array or similar\n        self.cache   = image_cache\n        self.labels  = labels\n        self.indices = indices\n        self.is_train = is_train\n\n    def __len__(self): return len(self.indices)\n\n    def __getitem__(self, idx):\n        i = int(self.indices[idx])\n        # uint8 [0,255] → float32 [0,1]\n        img = self.cache[i].float().div_(255.0)\n        if self.is_train:\n            img = self._aug(img)\n        img = self._normalize(img)\n        label = torch.tensor(self.labels[i], dtype=torch.float32)\n        return img, label\n\n\nprint('Model classes ready')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Stage 1 — Train MLP head on frozen features (fast, GPU)\n",
    "\n",
    "def stage1_train_mlp(train_idx, val_idx, config):\n",
    "    \"\"\"在快取特徵上訓練 MLP head，完全不需要 image loading\"\"\"\n",
    "    X_tr = torch.tensor(all_features[train_idx], dtype=torch.float32)\n",
    "    y_tr = torch.tensor(all_labels[train_idx],   dtype=torch.float32)\n",
    "    X_va = torch.tensor(all_features[val_idx],   dtype=torch.float32)\n",
    "    y_va = torch.tensor(all_labels[val_idx],     dtype=torch.float32)\n",
    "\n",
    "    tr_loader = DataLoader(TensorDataset(X_tr, y_tr),\n",
    "                           batch_size=config['lp_batch_size'], shuffle=True, drop_last=False)\n",
    "\n",
    "    head = build_mlp_head(X_tr.shape[1], config['lp_hidden'], config['lp_dropout']).to(device)\n",
    "    opt  = optim.AdamW(head.parameters(), lr=config['lp_lr'], weight_decay=config['lp_weight_decay'])\n",
    "    sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=config['lp_epochs'], eta_min=1e-6)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_r2 = -float('inf')\n",
    "    best_state  = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Ridge baseline for comparison\n",
    "    ridge = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0])\n",
    "    ridge.fit(all_features[train_idx], all_labels[train_idx])\n",
    "    ridge_r2 = r2_score(all_labels[val_idx], ridge.predict(all_features[val_idx]))\n",
    "    print(f'  Ridge baseline R2={ridge_r2:.4f}')\n",
    "\n",
    "    for epoch in range(config['lp_epochs']):\n",
    "        head.train()\n",
    "        for xb, yb in tr_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(head(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        sched.step()\n",
    "\n",
    "        head.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = head(X_va.to(device)).cpu().numpy()\n",
    "        val_r2 = r2_score(y_va.numpy(), val_preds)\n",
    "\n",
    "        if val_r2 > best_val_r2:\n",
    "            best_val_r2 = val_r2\n",
    "            best_state  = {k: v.cpu().clone() for k, v in head.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 30 == 0 or epoch == 0:\n",
    "            print(f'  LP Ep {epoch+1:3d}/{config[\"lp_epochs\"]} | val R2={val_r2:.4f} | best={best_val_r2:.4f}')\n",
    "\n",
    "        if patience_counter >= config['lp_patience']:\n",
    "            print(f'  LP early stop at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    print(f'  Stage 1 done: Ridge R2={ridge_r2:.4f} → MLP R2={best_val_r2:.4f} (delta={best_val_r2-ridge_r2:+.4f})')\n",
    "    del head; torch.cuda.empty_cache()\n",
    "    return best_state, ridge_r2, best_val_r2\n",
    "\n",
    "\n",
    "print('Stage 1 function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Stage 2 — Fine-tune backbone + MLP head\n\ndef stage2_finetune(fold_idx, train_idx, val_idx, head_state, lp_r2, config):\n    print(f'\\n--- Stage 2: Fine-Tune (backbone + MLP head) ---')\n\n    nw = config.get('ft_num_workers', 4)\n    # ← 用 image_cache (RAM) 取代 full_ds (磁碟)；消除 GPU idle from I/O\n    train_ds_obj = BeetleImageDataset(image_cache, all_labels, train_idx, is_train=True)\n    val_ds_obj   = BeetleImageDataset(image_cache, all_labels, val_idx,   is_train=False)\n    train_loader = DataLoader(train_ds_obj, batch_size=config['ft_batch_size'],\n                              shuffle=True,  num_workers=nw, pin_memory=True, drop_last=True,\n                              persistent_workers=(nw > 0), prefetch_factor=(4 if nw > 0 else None))\n    val_loader   = DataLoader(val_ds_obj,   batch_size=config['ft_batch_size'],\n                              shuffle=False, num_workers=nw, pin_memory=True,\n                              persistent_workers=(nw > 0), prefetch_factor=(4 if nw > 0 else None))\n\n    model = BioCLIP_MLP_LPFT(\n        hidden=config['lp_hidden'],\n        dropout=config['lp_dropout'],\n        head_state=head_state\n    ).to(device)\n\n    optimizer = torch.optim.AdamW([\n        {'params': model.bioclip.parameters(), 'lr': config['ft_lr_backbone']},\n        {'params': model.head.parameters(),    'lr': config['ft_lr_head']},\n    ], weight_decay=config['ft_weight_decay'])\n\n    total_steps  = len(train_loader) * config['ft_epochs']\n    warmup_steps = len(train_loader) * config['ft_warmup_epochs']\n    def lr_lambda(step):\n        if step < warmup_steps:\n            return step / max(warmup_steps, 1)\n        progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n        return 0.5 * (1 + math.cos(math.pi * progress))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    criterion = nn.MSELoss()\n    scaler = GradScaler()\n\n    best_val_loss = float('inf')\n    best_val_r2   = lp_r2  # start from Stage 1 baseline\n    patience_counter = 0\n    best_state = None\n\n    steps_per_epoch = len(train_loader)\n    print(f'  steps/epoch={steps_per_epoch}, batch={config[\"ft_batch_size\"]}, '\n          f'backbone_lr={config[\"ft_lr_backbone\"]:.0e}, head_lr={config[\"ft_lr_head\"]:.0e}')\n\n    for epoch in range(config['ft_epochs']):\n        model.train()\n        train_losses = []\n        t0 = time.time()\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            with autocast():\n                loss = criterion(model(images), labels)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            train_losses.append(loss.item())\n\n        model.eval()\n        vp, vl = [], []\n        with torch.no_grad():\n            for images, labels in val_loader:\n                with autocast():\n                    preds = model(images.to(device))\n                vp.append(preds.float().cpu())\n                vl.append(labels)\n\n        val_preds  = torch.cat(vp).numpy()\n        val_labels = torch.cat(vl).numpy()\n        val_loss   = np.mean((val_preds - val_labels) ** 2)\n        val_r2     = r2_score(val_labels, val_preds)\n\n        marker = ''\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_val_r2   = val_r2\n            patience_counter = 0\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            marker = f' <<< best'\n        else:\n            patience_counter += 1\n\n        print(f'  Ep {epoch+1:2d}/{config[\"ft_epochs\"]} | '\n              f'Train RMSE={np.sqrt(np.mean(train_losses)):.4f} | '\n              f'Val RMSE={np.sqrt(val_loss):.4f} R2={val_r2:.4f} | '\n              f'{time.time()-t0:.0f}s{marker}')\n\n        if patience_counter >= config['ft_patience']:\n            print(f'  Early stopping at epoch {epoch+1}')\n            break\n\n    if best_state is None:\n        print('  WARNING: FT did not improve Stage 1. Using Stage 1 state.')\n        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n\n    # ---- Save ----\n    fold_dir = f'{SAVE_DIR}/fold_{fold_idx + 1}'\n    os.makedirs(fold_dir, exist_ok=True)\n\n    model.load_state_dict(best_state)\n    model = model.to(device).eval()\n    torch.save(best_state, f'{fold_dir}/model.pth')\n\n    # JIT FP16\n    try:\n        dummy = torch.randn(1, 3, 224, 224).to(device)\n        with torch.no_grad():\n            traced = torch.jit.trace(model.half(), dummy.half())\n            jit_path = f'{fold_dir}/mlp_lpft_fold{fold_idx+1}_fp16.pt'\n            traced.save(jit_path)\n            print(f'  JIT FP16: {os.path.getsize(jit_path)/1e6:.0f} MB → {jit_path}')\n    except Exception as e:\n        print(f'  JIT failed: {e}')\n\n    # OOF predictions\n    model = model.float().to(device).eval()\n    oof_preds = []\n    with torch.no_grad():\n        for images, _ in val_loader:\n            with autocast():\n                preds = model(images.to(device))\n            oof_preds.append(preds.float().cpu().numpy())\n    oof_preds = np.concatenate(oof_preds)\n\n    del model, optimizer, scaler\n    torch.cuda.empty_cache()\n\n    return best_val_r2, np.sqrt(best_val_loss), oof_preds\n\n\nprint('Stage 2 function ready')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run all 5 folds\n",
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "\n",
    "kfold       = KFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "all_indices = np.arange(N)\n",
    "\n",
    "fold_results  = []\n",
    "all_oof_preds = np.zeros((N, 3))\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(all_indices)):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold_idx+1}/{CONFIG[\"n_folds\"]}')\n",
    "    print(f'Train: {len(train_idx)}, Val: {len(val_idx)}')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    # ---- Stage 1: MLP on frozen features ----\n",
    "    print('\\n--- Stage 1: MLP Linear Probe (frozen features) ---')\n",
    "    head_state, ridge_r2, lp_r2 = stage1_train_mlp(train_idx, val_idx, CONFIG)\n",
    "\n",
    "    # ---- Stage 2: Fine-tune backbone ----\n",
    "    ft_r2, ft_rmse, oof_preds = stage2_finetune(\n",
    "        fold_idx, train_idx, val_idx, head_state, lp_r2, CONFIG)\n",
    "\n",
    "    all_oof_preds[val_idx] = oof_preds\n",
    "\n",
    "    result = {\n",
    "        'fold': fold_idx + 1,\n",
    "        'ridge_r2': float(ridge_r2),\n",
    "        'lp_r2':    float(lp_r2),\n",
    "        'ft_r2':    float(ft_r2),\n",
    "        'ft_rmse':  float(ft_rmse),\n",
    "        'delta_lp_vs_ridge': float(lp_r2 - ridge_r2),\n",
    "        'delta_ft_vs_lp':    float(ft_r2 - lp_r2),\n",
    "    }\n",
    "    fold_results.append(result)\n",
    "    print(f'\\n  Fold {fold_idx+1} summary: Ridge={ridge_r2:.4f} → MLP-LP={lp_r2:.4f} → FT={ft_r2:.4f}')\n",
    "\n",
    "    # Checkpoint\n",
    "    np.save(f'{SAVE_DIR}/oof_preds_partial.npy', all_oof_preds)\n",
    "    with open(f'{SAVE_DIR}/partial_summary.json', 'w') as f:\n",
    "        json.dump({'completed_folds': fold_idx+1, 'results': fold_results}, f, indent=2)\n",
    "    print(f'  Checkpoint saved')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('ALL FOLDS COMPLETE')\n",
    "print(f'{\"=\"*60}')\n",
    "print(f'{\"Fold\":>6} {\"Ridge R2\":>10} {\"MLP-LP R2\":>10} {\"FT R2\":>10} {\"Δ(LP-Ridge)\":>12} {\"Δ(FT-LP)\":>10}')\n",
    "for r in fold_results:\n",
    "    print(f'{r[\"fold\"]:>6} {r[\"ridge_r2\"]:>10.4f} {r[\"lp_r2\"]:>10.4f} {r[\"ft_r2\"]:>10.4f} '\n",
    "          f'{r[\"delta_lp_vs_ridge\"]:>+12.4f} {r[\"delta_ft_vs_lp\"]:>+10.4f}')\n",
    "print(f'{\"Mean\":>6} '\n",
    "      f'{np.mean([r[\"ridge_r2\"] for r in fold_results]):>10.4f} '\n",
    "      f'{np.mean([r[\"lp_r2\"] for r in fold_results]):>10.4f} '\n",
    "      f'{np.mean([r[\"ft_r2\"] for r in fold_results]):>10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save OOF + compare vs V121\n",
    "np.save(f'{SAVE_DIR}/mlp_lpft_oof_preds.npy', all_oof_preds)\n",
    "np.save(f'{SAVE_DIR}/true_vals.npy', all_labels)\n",
    "\n",
    "oof_r2   = r2_score(all_labels, all_oof_preds)\n",
    "oof_rmse = np.sqrt(np.mean((all_labels - all_oof_preds) ** 2))\n",
    "\n",
    "print(f'\\n=== V123 MLP LP-FT OOF Results ===')\n",
    "print(f'  Overall R2:   {oof_r2:.4f}')\n",
    "print(f'  Overall RMSE: {oof_rmse:.4f}')\n",
    "for t, name in enumerate(TARGETS):\n",
    "    r2_t   = r2_score(all_labels[:, t], all_oof_preds[:, t])\n",
    "    rmse_t = np.sqrt(np.mean((all_labels[:, t] - all_oof_preds[:, t]) ** 2))\n",
    "    print(f'  {name}: R2={r2_t:.4f}, RMSE={rmse_t:.4f}')\n",
    "\n",
    "print(f'\\n=== Comparison ===')\n",
    "baselines = [\n",
    "    ('Frozen BioCLIP (V116)',   f'{PROJECT}/4_models/oof_predictions/bioclip_oof_preds.npy'),\n",
    "    ('LP-FT Linear (V121)',     f'{PROJECT}/4_models/v121_lpft_kfold/lpft_oof_preds.npy'),\n",
    "]\n",
    "for name, path in baselines:\n",
    "    try:\n",
    "        preds = np.load(path)\n",
    "        r2 = r2_score(all_labels, preds)\n",
    "        print(f'  {name}: R2={r2:.4f}')\n",
    "    except:\n",
    "        print(f'  {name}: not found')\n",
    "print(f'  MLP LP-FT (V123):       R2={oof_r2:.4f}  ← this')\n",
    "\n",
    "summary = {\n",
    "    'model': 'MLP LP-FT BioCLIP-2 (5-fold KFold)',\n",
    "    'config': CONFIG,\n",
    "    'overall_r2': float(oof_r2),\n",
    "    'overall_rmse': float(oof_rmse),\n",
    "    'fold_results': fold_results,\n",
    "}\n",
    "with open(f'{SAVE_DIR}/summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f'\\nAll saved to {SAVE_DIR}')\n",
    "\n",
    "if oof_r2 > 0.46:\n",
    "    print('\\n>>> R2 > 0.46! 加入 V124 stacking 有望提升分數。')\n",
    "elif oof_r2 > 0.42:\n",
    "    print('\\n>>> R2 與 V121 相近，MLP 帶來多樣性仍有價值。')\n",
    "else:\n",
    "    print('\\n>>> R2 偏低，考慮增加 hidden dim 或更多 FT epochs。')"
   ]
  }
 ]
}